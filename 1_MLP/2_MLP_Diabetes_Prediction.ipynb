{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict whether or not a patient has diabetes, based on certain diagnostic measurements\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases, USA. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "- The datasets consists of several medical predictor variables and one target variable, Outcome.\n",
    "\n",
    "Input Variables (X):\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "- Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = loadtxt('pima-indians-diabetes.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "X = data[:,0:8]\n",
    "y = data[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Keras Model\n",
    "\n",
    "- Models in Keras are defined as a sequence of layers.\n",
    "- We create a Sequential model and add layers one at a time until we are happy with our network architecture.\n",
    "- The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the `input_dim` argument and setting it to 8 for the 8 input variables.\n",
    "\n",
    "**How do we know the number of layers and their types?**\n",
    "\n",
    "- This is a very hard question. There are heuristics that we can use and often the best network structure is found through a process of trial and error experimentation (I explain more about this here). Generally, you need a network large enough to capture the structure of the problem.\n",
    "\n",
    "\n",
    "- In this example, we will use a fully-connected network structure with three layers.\n",
    "- Fully connected layers are defined using the Dense class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the `activation` argument.\n",
    "- We will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.\n",
    "- Better performance is achieved using the ReLU activation function. We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "- We will define the optimizer as the efficient stochastic gradient descent algorithm “adam“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Keras Model\n",
    "\n",
    "- Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU or GPU or even distributed.\n",
    "- When compiling, we must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in our dataset.\n",
    "- We must specify the loss function to use to evaluate a set of weights, the optimizer is used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "- In this case, we will use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "- We have defined our model and compiled it ready for efficient computation.\n",
    "- Now it is time to execute the model on some data.\n",
    "- We can train or fit our model on our loaded data by calling the fit() function on the model.\n",
    "- Training occurs over **epochs** and each epoch is split into **batches**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Sample?\n",
    "- A sample is a single row of data.\n",
    "- It contains inputs that are fed into the algorithm and an output that is used to compare to the prediction and calculate an error.\n",
    "- A training dataset is comprised of many rows of data or many samples.\n",
    "- A sample may also be called an instance, an observation, an input vector, or a feature vector.\n",
    "\n",
    "\n",
    "## What Is a Batch?\n",
    "- The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "- Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model.\n",
    "- A training dataset can be divided into one or more batches.\n",
    "- When all training samples are used to create one batch, the learning algorithm is called batch gradient descent. When the batch is the size of one sample, the learning algorithm is called stochastic gradient descent. When the batch size is more than one sample and less than the size of the training dataset, the learning algorithm is called mini-batch gradient descent.\n",
    "- In the case of mini-batch gradient descent, popular batch sizes include 32, 64, and 128 samples. You may see these values used in models in the literature and in tutorials.\n",
    "\n",
    "**What if the dataset does not divide evenly by the batch size?**\n",
    "\n",
    "- This can and does happen often when training a model. It simply means that the final batch has fewer samples than the other batches.\n",
    "- Alternately, you can remove some samples from the dataset or change the batch size such that the number of samples in the dataset does divide evenly by the batch size.\n",
    "\n",
    "## What Is an Epoch?\n",
    "- The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "- One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. For example, as above, an epoch that has one batch is called the batch gradient descent learning algorithm.\n",
    "- You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.\n",
    "- The number of epochs is traditionally large, often hundreds or thousands, allowing the learning algorithm to run until the error from the model has been sufficiently minimized.\n",
    "- It is common to create line plots that show epochs along the x-axis as time and the error or skill of the model on the y-axis. These plots are sometimes called **learning curves**. These plots can help to diagnose whether the model has over learned, under learned, or is suitably fit to the training dataset.\n",
    "\n",
    "## What Is the Difference Between Batch and Epoch?\n",
    "- The batch size is a number of samples processed before the model is updated.\n",
    "- The number of epochs is the number of complete passes through the training dataset.\n",
    "- The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n",
    "- The number of epochs can be set to an integer value between one and infinity. You can run the algorithm for as long as you like and even stop it using other criteria besides a fixed number of epochs, such as a change (or lack of change) in model error over time.\n",
    "- They are both integer values and they are both hyperparameters for the learning algorithm.\n",
    "- **There are no magic rules for how to configure these parameters. You must try different values and see what works best for given problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir = \"log_dir\", histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "history = model.fit(X, y, epochs=30, batch_size=10, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "ax[0].plot(history.epoch, history.history['loss'], label='training')\n",
    "ax[0].set_title('loss')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].plot(history.epoch,history.history['accuracy'], label='training')\n",
    "ax[1].set_title('accuracy')\n",
    "ax[1].legend(loc='best');\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tensorboard UI in this notebook itself\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir log_dir/ --bind_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
