{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning Project Lifecycle\n",
    "**Step 1: Define a problem<br>\n",
    "Step 2: Data gathering<br>\n",
    "Step 3: Exploratory Data Analysis<br>\n",
    "Step 4: Feature engineering and selection<br>\n",
    "Step 5: Data preparation for modelling** (train/test split)<br>\n",
    "**Step 6: Model Building<br>\n",
    "Step 7: Model Validation & Evaluation<br>\n",
    "Step 8: Model Tuning<br>\n",
    "Step 9: Model Deployment**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decision Tree - Example\n",
    "## Problem: Predicting risky bank loans using C5.0 decision trees\n",
    "\n",
    "The default vector indicates whether the loan applicant was unable to meet the agreed payment terms and went into default. A total of 30 percent of the loans in this dataset went into default. We have to train our model and predict such defaulters. \n",
    "\n",
    "## Data: \n",
    "1. checking_balance        - object\n",
    "2. months_loan_duration     - int64\n",
    "3. credit_history          - object\n",
    "4. purpose                 - object\n",
    "5. amount                   - int64\n",
    "6. savings_balance         - object\n",
    "7. employment_length       - object\n",
    "8. installment_rate         - int64\n",
    "9. personal_status         - object\n",
    "10. other_debtors           - object\n",
    "11. residence_history        - int64\n",
    "12. property                - object\n",
    "13. age                      - int64\n",
    "14. installment_plan        - object\n",
    "15. housing                 - object\n",
    "16. existing_credits         - int64\n",
    "17. job                     - object\n",
    "18. dependents               - int64\n",
    "19. telephone               - object\n",
    "20. foreign_worker          - object\n",
    "21. default                  - int64 (Target variable/Label)\n",
    "\n",
    "a) default = 1 --> Normal Customer<br>\n",
    "b) default = 2 --> Risky Customer/Deaulter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "pd.set_option('display.max_columns',30)\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit = pd.read_csv('credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit.isnull().sum() #checking NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit.checking_balance.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit['savings_balance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**How does correlation help in feature selection?**<br>\n",
    "Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5), dpi=150)\n",
    "sns.heatmap(credit.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find out the columns which are strings and cateogrical\n",
    "\n",
    "- Checking unique values in each column to find the categorical columns.\n",
    "- The description of data tells us which columns are categorical and which are continous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking unique values in each column, just to find the categorical columns.\n",
    "# Generally it is given in the description of data which columns are categorical and which are continous.\n",
    "credit.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LabelEncoder is used for converting categorical string columns to numeric.\n",
    "- Algorithms from sklearn do not accept input columns with string type, convert those columns to numerical. \n",
    " \n",
    " - So, we need to convert such columns (e.g. \"checking_balance\" or \"purpose\" in this dataset) into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Following coloumns are to be converted into srting\n",
    "categorical_cols = ['checking_balance','credit_history','purpose','savings_balance',\n",
    "                    'employment_length','personal_status','other_debtors','property',\n",
    "                    'installment_plan','housing', 'job', 'telephone', 'foreign_worker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LabelEncoder is used for converting categorical string columns to numeric.\n",
    "# Read more about LabelEncoder in sklearn documentation.\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    # Taking a column from dataframe, encoding it and replacing same column in the dataframe.\n",
    "    credit[col] = le.fit_transform(credit[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit.head()      # now all the string columns are converted into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Total customers/samples - 1000\n",
    "credit.shape # 1000 samples with 21 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Data - Selecting 900 rows at random from the dataframe for training\n",
    "credit_train = credit.sample(900, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Data - Taking remaining 100 rows for testing by dropping the rows present in train dataframe from original dataframe.\n",
    "credit_test = credit.drop(credit_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check whether this appears to be a fairly even split or not,\n",
    "# train should have about 30 percent of defaulted loans \n",
    "# and test data also should have similar % of default loans\n",
    "(credit.default.value_counts()/credit.default.count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train data - Ration of normal and risky customers\n",
    "(credit_train.default.value_counts()/credit_train.default.count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test data - Ration of normal and risky customers\n",
    "(credit_test.default.value_counts()/credit_test.default.count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#taking label in seperate objects\n",
    "train_labels = credit_train.default # y_train\n",
    "test_labels = credit_test.default # y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove Label column from train and test data sets\n",
    "credit_train.drop(\"default\", axis = 1, inplace=True) # X_train\n",
    "credit_test.drop(\"default\", axis = 1, inplace=True) # X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the model (Decison Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating object of the DT with required options \n",
    "model = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training/Build the model with train data\n",
    "model.fit(credit_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_graphviz(model,\n",
    "                 out_file=\"tree.dot\",\n",
    "                 feature_names = credit_train.columns, \n",
    "                 class_names=[\"1\",\"2\"],\n",
    "                 filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install Graphviz:\n",
    "https://graphviz.org/download/<br>\n",
    "\n",
    "Online Graphviz:<br>\n",
    "https://dreampuf.github.io/GraphvizOnline<br>\n",
    "http://www.webgraphviz.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data = credit_test.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data.values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- By reshaping array with `(-1, 1)`, the array gets reshaped in such a way that the resulting array has only 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(new_data.values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = model.predict(credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accuracy\n",
    "Accuracy is the proximity of measurement results to the true value. It tell us how accurate our classification model is able to predict the class labels given in the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_labels,predictions)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/894/1*bFkVvry-3YY_mHBwa5n8bQ.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once the model is ready to predict, we try making predictions on the test dataset. And once we segment the results into a matrix similar to as shown in the above figure, we can see how much our model is able to predict right and how much of its predictions are wrong.\n",
    "1. **TP (True-positives):** Where the actual label for that column was “Yes” in the test dataset and our model also predicted “Yes”.\n",
    "2. **TN (True-negatives):** Where the actual label for that column was “No” in the test dataset and our model also predicted “No”.\n",
    "3. **FP (False-positives):** Where the actual label for that column was “No” in the test dataset but our model predicted “Yes”.\n",
    "4. **FN (False-negatives):** Where the actual label for that column was “Yes” in the test dataset but our model predicted “No”.\n",
    "\n",
    "These 4 cells constitute the “Confusion matrix” as in the matrix which can alleviate all the confusion about the goodness of our model by painting a clear picture of our model’s predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Type I Error**\n",
    "\n",
    "A type 1 error is also known as a **false positive** and occurs when a classification model incorrectly predicts a true outcome for an originally false observation.\n",
    "\n",
    "**Type II Error**\n",
    "\n",
    "A type II error is also known as a **false negative** and occurs when a classification model incorrectly predicts a false outcome for an originally true observation.\n",
    "\n",
    "We should try to reduce Type 1 and/or Type 2 Errors as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/894/1*bFkVvry-3YY_mHBwa5n8bQ.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Scikit-learn sorts labels in ascending order, thus 1's are first column/row and 2's are the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Precision attempts to answer the following question:<br>\n",
    "What proportion of positive identifications was actually correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/292/1*NKFVmakz6V9jb_23gKUqaw.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Precision (P)\n",
    "precision_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Recall/ Sensitivity/ TPR (True Positive Rate) attempts to answer the following question:<br>\n",
    "What proportion of actual positives was identified correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/352/1*jJeDnEWUjbDLqjbl8WWDnQ.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recall (R)\n",
    "recall_score(test_labels,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- In some problem statements higher Recall takes precedence over a higher Precision and vice-versa.\n",
    "- But in some problem statements, where the distinction between Recall and Precision is not very clear and we want to give importance to both Recall and Precision, there is another metric- F1 Score that can be used. It is dependent on both Precision and Recall.\n",
    "\n",
    "- In a statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test’s accuracy. It considers both the precision p and the recall r of the test to compute the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"https://www.gstatic.com/education/formulas2/355397047/en/f1_score.svg\" width=40%></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# F1-Score\n",
    "f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Receiver Operating Characteristics (ROC) curve , Area under the Curve (AUC)**\n",
    "\n",
    "- To plot a ROC curve, we have to plot False Positive Rate on x-axis and Sensitivity i.e. True Positive Rate on the y-axis.\n",
    "\n",
    "- The area under the ROC curve is known as AUC. The more the AUC the better your model is. The farther away your ROC curve is from the middle linear line, the better your model is. This is how ROC-AUC can help us judge the performance of our classification models as well as provide us a means to select one model from many classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/6048/1*vlUaNZwMzoRsk2dBVNvnvQ.jpeg\" width=50%></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_labels, predictions, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roc_auc_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',lw=2,\n",
    "         label=f'ROC curve (area ={round(roc_auc_score(test_labels, predictions),2)})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credit_train.reset_index(drop = True, inplace=True)\n",
    "train_labels.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "cv_model = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "for i, index in enumerate(cv.split(credit_train)):\n",
    "    train_index = index[0]\n",
    "    test_index = index[1]\n",
    "    \n",
    "    print(\"Train Index: \", train_index, \"\\n\")\n",
    "    print(\"Test Index: \", test_index)\n",
    "\n",
    "    X_train = credit_train.loc[train_index]\n",
    "    y_train = train_labels[train_index]\n",
    "    \n",
    "    X_test = credit_train.loc[test_index]   \n",
    "    y_test = train_labels[test_index]\n",
    "    \n",
    "    cv_model.fit(X_train, y_train)\n",
    "    scores.append(cv_model.score(X_test, y_test))\n",
    "    joblib.dump(cv_model,f'models/DTmodel_{i}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = joblib.load(\"models/DTmodel_0.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(credit_test)\n",
    "accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Use other model evalution metrics to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
