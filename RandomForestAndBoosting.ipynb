{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5737c783-a9e1-4182-bba0-c3f1cf953358",
   "metadata": {},
   "source": [
    "# Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdcb440-fa29-4420-869b-30b466e013b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import f1_score, roc_auc_score,confusion_matrix, accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tabulate\n",
    "#from utils import predict_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb73875b-3476-41de-85c7-a6571132119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/media/abhishek/589E61B39E618A783/C-DAC Document/Practical Machine Learning/PML/Code/Dataset/CC.csv.bz2', compression = 'bz2', index_col = 0)\n",
    "df = pd.read_csv('/media/abhishek/589E61B39E618A783/C-DAC Document/Practical Machine Learning/PML/Code/Dataset/creditcard.csv')\n",
    "#df = pd.read_csv('CC.csv.bz2',\n",
    " #                compression='bz2',\n",
    "  #               index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777d956c-fcf8-47b3-b505-b3956a5e374e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55409d47-dd89-4254-8ed0-83925367cced",
   "metadata": {},
   "source": [
    "## Understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943ad8d7-32b0-4e47-b6ed-83bbf5279325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee768d17-dc9a-4c2c-ab0c-ec9431550acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6899bae-068a-4ba2-b22a-6b60c30825fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbc6d1-93ed-41f0-a9e4-80939eeab61e",
   "metadata": {},
   "source": [
    "### check distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c9d76c-bcaf-4fba-9bfb-b09e4d544416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Fraud :  99.83 % of the dataset\n",
      "Frauds :  0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Non Fraud : \", round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print(\"Frauds : \", round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e734ba-537e-46ef-9a27-29c22b23f441",
   "metadata": {},
   "source": [
    "### The classes are haveily skewed. This is problem that needs to be solved. How ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da738196-ecfb-4e09-9b42-1f4bf7de3936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Fraud  284315  are normal transaction\n",
      "Frauds  492  are fraud\n"
     ]
    }
   ],
   "source": [
    "print(\"Non Fraud \", round(df['Class'].value_counts()[0],2), ' are normal transaction')\n",
    "print(\"Frauds \", round(df['Class'].value_counts()[1],2), ' are fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb071d29-6042-41c7-87fd-1e878e67cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'class Distribution \\n (0: No Fraud || 1 : Fraud)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('Class', data = df, palette = colors)\n",
    "plt.title('class Distribution \\n (0: No Fraud || 1 : Fraud)', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df37c5-ec62-4364-b993-2ac8d6ce21a7",
   "metadata": {},
   "source": [
    "- Notice how imbalanced is our original dataset!\n",
    "- Most of the transactions are non-fraud.\n",
    "- If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud.\n",
    "- But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a3aff-4aaf-4525-8010-fc820b60fc70",
   "metadata": {},
   "source": [
    "## Pre-Processing - Scaling and Distribution\n",
    " - We will first scale the columns comprise of <b>Time</b> and <b>Amount</b> .\n",
    " - Time and amount should be scaled as the other columns.\n",
    " - On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c903be-14d2-403a-8f68-1533094eecb2",
   "metadata": {},
   "source": [
    "## What is a sub-Sample?\n",
    " - In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892ca59-4beb-4990-a3fd-214c32dbb610",
   "metadata": {},
   "source": [
    "## Why do we create a sub-Sample?\n",
    "We saw that the original dataframe is heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
    "<ul>\n",
    "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
    "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba1293-3272-41b7-bc7a-2e72ee36620a",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "The **StandardScaler** assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. \n",
    "\n",
    "$$\\frac{\\text{x}-\\text{mean}}{\\text{standard deviation}}$$\n",
    "\n",
    "The **MinMaxScaler** is the probably the most famous scaling algorithm, and follows the following formula for each feature. \n",
    "\n",
    "$$\\frac{\\text{x}-\\text{min}}{\\text{max}-\\text{min}}$$\n",
    "\n",
    "It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better. However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler below.\n",
    "\n",
    "**Robust Scaler** scale features using statistics that are robust to outliers. The RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. \n",
    "\n",
    "$$\\frac{\\text{x}-\\text{Q1(x)}}{\\text{Q3(x)}-\\text{Q1(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9eed84-f9ca-4247-86fe-c069063ff1b0",
   "metadata": {},
   "source": [
    "- For Amount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59507aaf-73fd-4dc6-a1b8-6cc377b4b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df.drop(['Amount'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4bcb7a-c808-4453-8a58-ab145517927d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>1.783274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.269825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>4.983721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>1.418291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Class  scaled_amount  \n",
       "0 -0.189115  0.133558 -0.021053      0       1.783274  \n",
       "1  0.125895 -0.008983  0.014724      0      -0.269825  \n",
       "2 -0.139097 -0.055353 -0.059752      0       4.983721  \n",
       "3 -0.221929  0.062723  0.061458      0       1.418291  \n",
       "4  0.502292  0.219422  0.215153      0       0.670579  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbaa5b-abd8-42b2-a0bb-8328fae1d3b0",
   "metadata": {},
   "source": [
    "#### Rearranging the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "160201a6-8628-4f34-bef6-01cd01d817ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "df.drop(['scaled_amount'], axis = 1, inplace = True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132a8a4b-f9e2-4201-8937-74e0b671eb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  Time        V1        V2        V3        V4        V5  \\\n",
       "0       1.783274   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321   \n",
       "1      -0.269825   0.0  1.191857  0.266151  0.166480  0.448154  0.060018   \n",
       "2       4.983721   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198   \n",
       "3       1.418291   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4       0.670579   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193   \n",
       "\n",
       "         V6        V7        V8  ...       V20       V21       V22       V23  \\\n",
       "0  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13acdd-961b-49a1-a0d3-28e0b716f67e",
   "metadata": {},
   "source": [
    "- For Time Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8380114-550c-4bd0-85b5-a5cc358c5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "df.drop(['Time'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "080a5275-794f-4aa5-8fd6-97a877035697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount        V1        V2        V3        V4        V5        V6  \\\n",
       "0       1.783274 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      -0.269825  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       4.983721 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       1.418291 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       0.670579 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  scaled_time  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0    -0.994983  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0    -0.994983  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0    -0.994972  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0    -0.994972  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0    -0.994960  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131438fc-5425-4a96-a8c8-75c53e230efa",
   "metadata": {},
   "source": [
    "#### Rearranging the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f260cff2-bbc3-49ed-a1a1-b8db6c560836",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_time = df['scaled_time']\n",
    "df.drop(['scaled_time'], axis = 1, inplace = True)\n",
    "df.insert(0, 'scaled_time', scaled_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fddb15b-edb5-438f-8e97-c8b5cccc2dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_time  scaled_amount        V1        V2        V3        V4  \\\n",
       "0    -0.994983       1.783274 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1    -0.994983      -0.269825  1.191857  0.266151  0.166480  0.448154   \n",
       "2    -0.994972       4.983721 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3    -0.994972       1.418291 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4    -0.994960       0.670579 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e6f22-0a08-4652-beaf-f55f2bc2e936",
   "metadata": {},
   "source": [
    "### Splitting the DataFrame\n",
    "\n",
    "Before proceeding with any <b> Sampling technique</b> we have to separate the orginal dataframe.<br> \n",
    "<b> Why? for testing purposes, we want to test our models on the original testing set not on the testing set created by either of these techniques.</b><br> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3284c35-3344-4753-a4fc-c398e8696e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits = 1,\n",
    "                           test_size = 0.2,\n",
    "                           train_size = 0.8,\n",
    "                           random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7efa2e07-740d-46fb-8286-be8cc166279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('Class', axis = 1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82ba41d-4f53-4ea3-b908-9870df1413a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in ss.split(x, y):\n",
    "    train_df = df.iloc[train_index]\n",
    "    test_df = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "290950c4-699a-4fba-9ec7-49572e252287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221234</th>\n",
       "      <td>0.678979</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>2.126322</td>\n",
       "      <td>-0.010040</td>\n",
       "      <td>-1.997102</td>\n",
       "      <td>0.276477</td>\n",
       "      <td>0.639165</td>\n",
       "      <td>-0.755860</td>\n",
       "      <td>0.445171</td>\n",
       "      <td>-0.305989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275343</td>\n",
       "      <td>-0.048907</td>\n",
       "      <td>0.047566</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.335408</td>\n",
       "      <td>0.420944</td>\n",
       "      <td>-0.226205</td>\n",
       "      <td>-0.045156</td>\n",
       "      <td>-0.066126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53575</th>\n",
       "      <td>-0.453964</td>\n",
       "      <td>-0.135401</td>\n",
       "      <td>1.100318</td>\n",
       "      <td>0.177618</td>\n",
       "      <td>0.212730</td>\n",
       "      <td>0.966039</td>\n",
       "      <td>-0.232896</td>\n",
       "      <td>-0.122320</td>\n",
       "      <td>-0.236608</td>\n",
       "      <td>0.158837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085843</td>\n",
       "      <td>-0.084915</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>-0.056042</td>\n",
       "      <td>0.169951</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.410297</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.032014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163269</th>\n",
       "      <td>0.365535</td>\n",
       "      <td>-0.125900</td>\n",
       "      <td>-0.565880</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>1.400824</td>\n",
       "      <td>-2.647515</td>\n",
       "      <td>0.858462</td>\n",
       "      <td>-0.121119</td>\n",
       "      <td>0.654774</td>\n",
       "      <td>-0.243942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272824</td>\n",
       "      <td>-0.330358</td>\n",
       "      <td>-1.037985</td>\n",
       "      <td>-0.215048</td>\n",
       "      <td>-0.015207</td>\n",
       "      <td>0.432703</td>\n",
       "      <td>-0.668484</td>\n",
       "      <td>-0.170773</td>\n",
       "      <td>-0.167245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246069</th>\n",
       "      <td>0.802770</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>-1.610536</td>\n",
       "      <td>1.450172</td>\n",
       "      <td>-1.164202</td>\n",
       "      <td>-1.269844</td>\n",
       "      <td>1.317395</td>\n",
       "      <td>2.602690</td>\n",
       "      <td>-0.595004</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.844192</td>\n",
       "      <td>1.565997</td>\n",
       "      <td>-0.560198</td>\n",
       "      <td>0.291749</td>\n",
       "      <td>-1.730249</td>\n",
       "      <td>-0.701429</td>\n",
       "      <td>0.344150</td>\n",
       "      <td>-0.134201</td>\n",
       "      <td>0.013732</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198872</th>\n",
       "      <td>0.563905</td>\n",
       "      <td>-0.200796</td>\n",
       "      <td>-1.124829</td>\n",
       "      <td>1.615882</td>\n",
       "      <td>-0.246758</td>\n",
       "      <td>-1.007567</td>\n",
       "      <td>0.794176</td>\n",
       "      <td>-0.388234</td>\n",
       "      <td>0.852613</td>\n",
       "      <td>0.162459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123248</td>\n",
       "      <td>-0.252198</td>\n",
       "      <td>-0.636616</td>\n",
       "      <td>-0.118839</td>\n",
       "      <td>-1.025058</td>\n",
       "      <td>-0.115831</td>\n",
       "      <td>0.224957</td>\n",
       "      <td>0.035447</td>\n",
       "      <td>0.208210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>-0.621283</td>\n",
       "      <td>-0.296793</td>\n",
       "      <td>-0.866737</td>\n",
       "      <td>0.793163</td>\n",
       "      <td>0.791631</td>\n",
       "      <td>-0.019310</td>\n",
       "      <td>1.675213</td>\n",
       "      <td>4.374922</td>\n",
       "      <td>-0.837459</td>\n",
       "      <td>1.629948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027757</td>\n",
       "      <td>-0.138033</td>\n",
       "      <td>-0.418125</td>\n",
       "      <td>-0.257635</td>\n",
       "      <td>1.004318</td>\n",
       "      <td>0.445748</td>\n",
       "      <td>-0.304825</td>\n",
       "      <td>0.010633</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226406</th>\n",
       "      <td>0.703967</td>\n",
       "      <td>1.087543</td>\n",
       "      <td>-0.897157</td>\n",
       "      <td>-0.305330</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>-2.283739</td>\n",
       "      <td>-0.794812</td>\n",
       "      <td>0.796014</td>\n",
       "      <td>-0.092902</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620535</td>\n",
       "      <td>-0.413706</td>\n",
       "      <td>-0.415258</td>\n",
       "      <td>0.075932</td>\n",
       "      <td>0.205978</td>\n",
       "      <td>-0.592265</td>\n",
       "      <td>-0.436177</td>\n",
       "      <td>-0.638159</td>\n",
       "      <td>-0.111313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23564</th>\n",
       "      <td>-0.609382</td>\n",
       "      <td>-0.257249</td>\n",
       "      <td>-0.459635</td>\n",
       "      <td>0.752513</td>\n",
       "      <td>1.754365</td>\n",
       "      <td>0.571020</td>\n",
       "      <td>0.284946</td>\n",
       "      <td>-0.071830</td>\n",
       "      <td>0.366709</td>\n",
       "      <td>0.124772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085457</td>\n",
       "      <td>-0.051137</td>\n",
       "      <td>-0.108876</td>\n",
       "      <td>0.133992</td>\n",
       "      <td>0.106714</td>\n",
       "      <td>-0.720905</td>\n",
       "      <td>0.213609</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>0.140501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88109</th>\n",
       "      <td>-0.266709</td>\n",
       "      <td>2.040103</td>\n",
       "      <td>0.943566</td>\n",
       "      <td>-1.034119</td>\n",
       "      <td>0.974853</td>\n",
       "      <td>0.648536</td>\n",
       "      <td>-1.377906</td>\n",
       "      <td>0.329313</td>\n",
       "      <td>-0.844338</td>\n",
       "      <td>0.192469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206620</td>\n",
       "      <td>-0.262192</td>\n",
       "      <td>-0.556690</td>\n",
       "      <td>-0.103136</td>\n",
       "      <td>-0.036042</td>\n",
       "      <td>0.216285</td>\n",
       "      <td>-0.409985</td>\n",
       "      <td>0.055953</td>\n",
       "      <td>0.057321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42982</th>\n",
       "      <td>-0.509522</td>\n",
       "      <td>-0.167819</td>\n",
       "      <td>1.128656</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>1.430730</td>\n",
       "      <td>1.419232</td>\n",
       "      <td>-1.118919</td>\n",
       "      <td>-0.387504</td>\n",
       "      <td>-0.530623</td>\n",
       "      <td>0.050947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124404</td>\n",
       "      <td>-0.036928</td>\n",
       "      <td>0.112550</td>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.717788</td>\n",
       "      <td>0.381047</td>\n",
       "      <td>-0.429976</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>0.042288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227845 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_time  scaled_amount        V1        V2        V3        V4  \\\n",
       "221234     0.678979      -0.293440  2.126322 -0.010040 -1.997102  0.276477   \n",
       "53575     -0.453964      -0.135401  1.100318  0.177618  0.212730  0.966039   \n",
       "163269     0.365535      -0.125900 -0.565880  0.135593  1.400824 -2.647515   \n",
       "246069     0.802770       0.009642 -1.610536  1.450172 -1.164202 -1.269844   \n",
       "198872     0.563905      -0.200796 -1.124829  1.615882 -0.246758 -1.007567   \n",
       "...             ...            ...       ...       ...       ...       ...   \n",
       "21691     -0.621283      -0.296793 -0.866737  0.793163  0.791631 -0.019310   \n",
       "226406     0.703967       1.087543 -0.897157 -0.305330  0.001853 -2.283739   \n",
       "23564     -0.609382      -0.257249 -0.459635  0.752513  1.754365  0.571020   \n",
       "88109     -0.266709       2.040103  0.943566 -1.034119  0.974853  0.648536   \n",
       "42982     -0.509522      -0.167819  1.128656 -0.001649  1.430730  1.419232   \n",
       "\n",
       "              V5        V6        V7        V8  ...       V20       V21  \\\n",
       "221234  0.639165 -0.755860  0.445171 -0.305989  ... -0.275343 -0.048907   \n",
       "53575  -0.232896 -0.122320 -0.236608  0.158837  ... -0.085843 -0.084915   \n",
       "163269  0.858462 -0.121119  0.654774 -0.243942  ...  0.272824 -0.330358   \n",
       "246069  1.317395  2.602690 -0.595004 -0.075000  ... -0.844192  1.565997   \n",
       "198872  0.794176 -0.388234  0.852613  0.162459  ...  0.123248 -0.252198   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "21691   1.675213  4.374922 -0.837459  1.629948  ...  0.027757 -0.138033   \n",
       "226406 -0.794812  0.796014 -0.092902  0.030927  ... -0.620535 -0.413706   \n",
       "23564   0.284946 -0.071830  0.366709  0.124772  ...  0.085457 -0.051137   \n",
       "88109  -1.377906  0.329313 -0.844338  0.192469  ... -0.206620 -0.262192   \n",
       "42982  -1.118919 -0.387504 -0.530623  0.050947  ... -0.124404 -0.036928   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "221234  0.047566  0.023506  0.335408  0.420944 -0.226205 -0.045156 -0.066126   \n",
       "53575   0.007497 -0.056042  0.169951  0.425441  0.410297  0.015957  0.032014   \n",
       "163269 -1.037985 -0.215048 -0.015207  0.432703 -0.668484 -0.170773 -0.167245   \n",
       "246069 -0.560198  0.291749 -1.730249 -0.701429  0.344150 -0.134201  0.013732   \n",
       "198872 -0.636616 -0.118839 -1.025058 -0.115831  0.224957  0.035447  0.208210   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "21691  -0.418125 -0.257635  1.004318  0.445748 -0.304825  0.010633  0.020619   \n",
       "226406 -0.415258  0.075932  0.205978 -0.592265 -0.436177 -0.638159 -0.111313   \n",
       "23564  -0.108876  0.133992  0.106714 -0.720905  0.213609  0.140315  0.140501   \n",
       "88109  -0.556690 -0.103136 -0.036042  0.216285 -0.409985  0.055953  0.057321   \n",
       "42982   0.112550  0.031364  0.717788  0.381047 -0.429976  0.074866  0.042288   \n",
       "\n",
       "        Class  \n",
       "221234      0  \n",
       "53575       0  \n",
       "163269      0  \n",
       "246069      0  \n",
       "198872      0  \n",
       "...       ...  \n",
       "21691       0  \n",
       "226406      0  \n",
       "23564       0  \n",
       "88109       0  \n",
       "42982       0  \n",
       "\n",
       "[227845 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c818265-b048-4b94-8323-850828820b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions: \n",
      "\n",
      "Train Set\n",
      "0    227451\n",
      "1       394\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Test Set\n",
      "0    56864\n",
      "1       98\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Percentage:\n",
      "\n",
      "Train Set\n",
      "0    99.827075\n",
      "1     0.172925\n",
      "Name: Class, dtype: float64\n",
      "\n",
      "Test Set\n",
      "0    99.827955\n",
      "1     0.172045\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distributions: \\n')\n",
    "print(\"Train Set\")\n",
    "print(train_df.Class.value_counts())\n",
    "print(\"\\nTest Set\")\n",
    "print(test_df.Class.value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(\"\\nTrain Set\")\n",
    "print((train_df.Class.value_counts()/ len(train_df))*100)\n",
    "print(\"\\nTest Set\")\n",
    "print((test_df.Class.value_counts()/ len(test_df))*100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f33e574a-6f10-49f0-ad1c-a62a4b06e093",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6779d4c0-da09-45af-9925-d5071ff2368c",
   "metadata": {},
   "source": [
    "### Random Under-Sampling:\n",
    "\n",
    "Implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n",
    "\n",
    "**Steps:**\n",
    "<ul>\n",
    "<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n",
    "<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n",
    "<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n",
    "</ul>\n",
    "\n",
    "**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (randomly picking 394 non-fraud transaction  from 2,27,451 non-fraud transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "187182ed-7b83-44bc-959b-dd8c4d7e72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0800e979-2a37-4476-a86b-58ebea188146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = train_df.loc[train_df['Class'] == 1]\n",
    "non_fraud_df = train_df.loc[train_df['Class'] == 0][:394]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de549530-4aba-420e-ab8a-908ac4f07b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 31)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_df.shape\n",
    "non_fraud_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bbaba84-933a-40a8-9b44-22069d70a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = pd.concat([fraud_df, non_fraud_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a9ff70c-04e4-49a1-bc56-aaa8672d5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = normal_df.sample(frac = 1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67cff563-7206-4b37-bceb-bd5bec20ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110105</th>\n",
       "      <td>-0.152880</td>\n",
       "      <td>2.459303</td>\n",
       "      <td>1.075168</td>\n",
       "      <td>-1.187054</td>\n",
       "      <td>-0.602560</td>\n",
       "      <td>-0.674653</td>\n",
       "      <td>-0.537653</td>\n",
       "      <td>-0.020799</td>\n",
       "      <td>-0.153052</td>\n",
       "      <td>-0.077158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114433</td>\n",
       "      <td>-0.367976</td>\n",
       "      <td>-0.956913</td>\n",
       "      <td>-0.320144</td>\n",
       "      <td>-0.790730</td>\n",
       "      <td>0.461696</td>\n",
       "      <td>1.128829</td>\n",
       "      <td>-0.115607</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74507</th>\n",
       "      <td>-0.341569</td>\n",
       "      <td>1.515266</td>\n",
       "      <td>-7.427924</td>\n",
       "      <td>2.948209</td>\n",
       "      <td>-8.678550</td>\n",
       "      <td>5.185303</td>\n",
       "      <td>-4.761090</td>\n",
       "      <td>-0.957095</td>\n",
       "      <td>-7.773380</td>\n",
       "      <td>0.717309</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123085</td>\n",
       "      <td>-0.299847</td>\n",
       "      <td>0.610479</td>\n",
       "      <td>0.789023</td>\n",
       "      <td>-0.564512</td>\n",
       "      <td>0.201196</td>\n",
       "      <td>-0.111225</td>\n",
       "      <td>1.144599</td>\n",
       "      <td>0.102280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56703</th>\n",
       "      <td>-0.436413</td>\n",
       "      <td>-0.296793</td>\n",
       "      <td>1.176716</td>\n",
       "      <td>0.557091</td>\n",
       "      <td>-0.490800</td>\n",
       "      <td>0.756424</td>\n",
       "      <td>0.249192</td>\n",
       "      <td>-0.781871</td>\n",
       "      <td>0.228750</td>\n",
       "      <td>-0.040840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102772</td>\n",
       "      <td>-0.062166</td>\n",
       "      <td>-0.128168</td>\n",
       "      <td>-0.040176</td>\n",
       "      <td>0.110040</td>\n",
       "      <td>0.437891</td>\n",
       "      <td>0.368809</td>\n",
       "      <td>-0.018287</td>\n",
       "      <td>0.031173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42009</th>\n",
       "      <td>-0.514257</td>\n",
       "      <td>1.262209</td>\n",
       "      <td>-2.740483</td>\n",
       "      <td>3.658095</td>\n",
       "      <td>-4.110636</td>\n",
       "      <td>5.340242</td>\n",
       "      <td>-2.666775</td>\n",
       "      <td>-0.092782</td>\n",
       "      <td>-4.388699</td>\n",
       "      <td>-0.280133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185325</td>\n",
       "      <td>2.417495</td>\n",
       "      <td>-0.097712</td>\n",
       "      <td>0.382155</td>\n",
       "      <td>-0.154757</td>\n",
       "      <td>-0.403956</td>\n",
       "      <td>0.277895</td>\n",
       "      <td>0.830062</td>\n",
       "      <td>0.218690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150687</th>\n",
       "      <td>0.108143</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-10.300820</td>\n",
       "      <td>6.483095</td>\n",
       "      <td>-15.076363</td>\n",
       "      <td>6.554191</td>\n",
       "      <td>-8.880252</td>\n",
       "      <td>-4.471672</td>\n",
       "      <td>-14.900689</td>\n",
       "      <td>3.840170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113244</td>\n",
       "      <td>1.508748</td>\n",
       "      <td>1.041642</td>\n",
       "      <td>-0.682790</td>\n",
       "      <td>0.573544</td>\n",
       "      <td>-1.602389</td>\n",
       "      <td>-0.393521</td>\n",
       "      <td>-0.468893</td>\n",
       "      <td>0.105920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145800</th>\n",
       "      <td>0.029488</td>\n",
       "      <td>5.998323</td>\n",
       "      <td>-0.419820</td>\n",
       "      <td>-1.155978</td>\n",
       "      <td>-2.092516</td>\n",
       "      <td>2.786750</td>\n",
       "      <td>0.736297</td>\n",
       "      <td>-0.167292</td>\n",
       "      <td>1.600027</td>\n",
       "      <td>-0.117427</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275358</td>\n",
       "      <td>0.480640</td>\n",
       "      <td>0.533517</td>\n",
       "      <td>1.284645</td>\n",
       "      <td>0.516131</td>\n",
       "      <td>-0.602941</td>\n",
       "      <td>-0.305024</td>\n",
       "      <td>-0.021363</td>\n",
       "      <td>0.129096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261056</th>\n",
       "      <td>0.882905</td>\n",
       "      <td>0.328513</td>\n",
       "      <td>-0.408111</td>\n",
       "      <td>3.132944</td>\n",
       "      <td>-3.098030</td>\n",
       "      <td>5.803893</td>\n",
       "      <td>0.890609</td>\n",
       "      <td>-0.501474</td>\n",
       "      <td>-0.440054</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499568</td>\n",
       "      <td>0.098482</td>\n",
       "      <td>-0.538375</td>\n",
       "      <td>-0.217989</td>\n",
       "      <td>-1.042657</td>\n",
       "      <td>0.314389</td>\n",
       "      <td>0.543244</td>\n",
       "      <td>0.233851</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204079</th>\n",
       "      <td>0.592230</td>\n",
       "      <td>1.208831</td>\n",
       "      <td>1.862102</td>\n",
       "      <td>-0.124052</td>\n",
       "      <td>-1.989752</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.473032</td>\n",
       "      <td>-0.674517</td>\n",
       "      <td>0.298621</td>\n",
       "      <td>-0.282416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150727</td>\n",
       "      <td>-0.204158</td>\n",
       "      <td>-0.511441</td>\n",
       "      <td>0.077874</td>\n",
       "      <td>0.388335</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>-0.120980</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66967</th>\n",
       "      <td>-0.380503</td>\n",
       "      <td>0.477887</td>\n",
       "      <td>1.176609</td>\n",
       "      <td>-0.418367</td>\n",
       "      <td>0.727881</td>\n",
       "      <td>0.544218</td>\n",
       "      <td>-0.920313</td>\n",
       "      <td>-0.194964</td>\n",
       "      <td>-0.517127</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.481306</td>\n",
       "      <td>-0.499104</td>\n",
       "      <td>-1.071833</td>\n",
       "      <td>0.185201</td>\n",
       "      <td>-0.011466</td>\n",
       "      <td>0.082652</td>\n",
       "      <td>-0.580939</td>\n",
       "      <td>0.066921</td>\n",
       "      <td>0.044595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150925</th>\n",
       "      <td>0.111009</td>\n",
       "      <td>0.328093</td>\n",
       "      <td>-13.512074</td>\n",
       "      <td>8.215177</td>\n",
       "      <td>-16.582606</td>\n",
       "      <td>6.207369</td>\n",
       "      <td>-11.318472</td>\n",
       "      <td>-2.997207</td>\n",
       "      <td>-17.640470</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694574</td>\n",
       "      <td>-0.907906</td>\n",
       "      <td>1.514028</td>\n",
       "      <td>-0.141879</td>\n",
       "      <td>0.789186</td>\n",
       "      <td>-0.031343</td>\n",
       "      <td>-0.255057</td>\n",
       "      <td>-1.865831</td>\n",
       "      <td>-0.442204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>788 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_time  scaled_amount         V1        V2         V3        V4  \\\n",
       "110105    -0.152880       2.459303   1.075168 -1.187054  -0.602560 -0.674653   \n",
       "74507     -0.341569       1.515266  -7.427924  2.948209  -8.678550  5.185303   \n",
       "56703     -0.436413      -0.296793   1.176716  0.557091  -0.490800  0.756424   \n",
       "42009     -0.514257       1.262209  -2.740483  3.658095  -4.110636  5.340242   \n",
       "150687     0.108143      -0.293440 -10.300820  6.483095 -15.076363  6.554191   \n",
       "...             ...            ...        ...       ...        ...       ...   \n",
       "145800     0.029488       5.998323  -0.419820 -1.155978  -2.092516  2.786750   \n",
       "261056     0.882905       0.328513  -0.408111  3.132944  -3.098030  5.803893   \n",
       "204079     0.592230       1.208831   1.862102 -0.124052  -1.989752  0.382609   \n",
       "66967     -0.380503       0.477887   1.176609 -0.418367   0.727881  0.544218   \n",
       "150925     0.111009       0.328093 -13.512074  8.215177 -16.582606  6.207369   \n",
       "\n",
       "               V5        V6         V7        V8  ...       V20       V21  \\\n",
       "110105  -0.537653 -0.020799  -0.153052 -0.077158  ... -0.114433 -0.367976   \n",
       "74507   -4.761090 -0.957095  -7.773380  0.717309  ... -0.123085 -0.299847   \n",
       "56703    0.249192 -0.781871   0.228750 -0.040840  ... -0.102772 -0.062166   \n",
       "42009   -2.666775 -0.092782  -4.388699 -0.280133  ...  0.185325  2.417495   \n",
       "150687  -8.880252 -4.471672 -14.900689  3.840170  ...  0.113244  1.508748   \n",
       "...           ...       ...        ...       ...  ...       ...       ...   \n",
       "145800   0.736297 -0.167292   1.600027 -0.117427  ...  1.275358  0.480640   \n",
       "261056   0.890609 -0.501474  -0.440054  0.591828  ...  0.499568  0.098482   \n",
       "204079   0.473032 -0.674517   0.298621 -0.282416  ...  0.150727 -0.204158   \n",
       "66967   -0.920313 -0.194964  -0.517127  0.023388  ... -0.481306 -0.499104   \n",
       "150925 -11.318472 -2.997207 -17.640470  0.040349  ...  0.694574 -0.907906   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "110105 -0.956913 -0.320144 -0.790730  0.461696  1.128829 -0.115607  0.007571   \n",
       "74507   0.610479  0.789023 -0.564512  0.201196 -0.111225  1.144599  0.102280   \n",
       "56703  -0.128168 -0.040176  0.110040  0.437891  0.368809 -0.018287  0.031173   \n",
       "42009  -0.097712  0.382155 -0.154757 -0.403956  0.277895  0.830062  0.218690   \n",
       "150687  1.041642 -0.682790  0.573544 -1.602389 -0.393521 -0.468893  0.105920   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "145800  0.533517  1.284645  0.516131 -0.602941 -0.305024 -0.021363  0.129096   \n",
       "261056 -0.538375 -0.217989 -1.042657  0.314389  0.543244  0.233851  0.119603   \n",
       "204079 -0.511441  0.077874  0.388335  0.007896 -0.120980 -0.019579  0.006155   \n",
       "66967  -1.071833  0.185201 -0.011466  0.082652 -0.580939  0.066921  0.044595   \n",
       "150925  1.514028 -0.141879  0.789186 -0.031343 -0.255057 -1.865831 -0.442204   \n",
       "\n",
       "        Class  \n",
       "110105      0  \n",
       "74507       1  \n",
       "56703       1  \n",
       "42009       1  \n",
       "150687      1  \n",
       "...       ...  \n",
       "145800      1  \n",
       "261056      1  \n",
       "204079      1  \n",
       "66967       0  \n",
       "150925      1  \n",
       "\n",
       "[788 rows x 31 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270586b-1fba-4cbe-a146-66fdc162f4ee",
   "metadata": {},
   "source": [
    "###  Equally Distributing \n",
    "<a id=\"correlating\"></a>\n",
    "Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23f862ec-5b76-4c71-93bd-f48b347a0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df2['Class'].value_counts()/len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02e3a8ee-6dfb-41df-9750-073de18ae085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'class Distribution \\n (0: No Fraud || 1 : Fraud)')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"Black\", \"Yellow\"]\n",
    "\n",
    "sns.countplot('Class', data = df2, palette = colors)\n",
    "plt.title('class Distribution \\n (0: No Fraud || 1 : Fraud)', fontsize = 14) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ed638-a471-451e-b433-3487c1e50dac",
   "metadata": {},
   "source": [
    "### Training the ML Model for Fraud Detection(Classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8be082b-ba56-4b0f-8438-cdcc89964330",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df2.drop('Class', axis = 1)\n",
    "y_train = df2['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfa4fe57-e220-4d06-b6f4-b7ad68a45c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df.drop('Class', axis = 1)\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c1c51-120a-4374-8188-7c0a59e7d54b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc1db016-90ab-40c5-ac3f-51cda551a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b27b7dd2-9020-4e99-a4ba-9c7c60d4bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fad37aab-dfd8-4940-ad25-b9d09b82237b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=102678936),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2087038197),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1763646380),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=884351046),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1886717520),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1923326233),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=461524449),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1003707483),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2063930840),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=79019908),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=203270738),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1138728473),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1611748107),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=940606716),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=414165688),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2070669955),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1208856038),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=724691160),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2040495372),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1640790505),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1568696764),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=869886535),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1188405701),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2004358823),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1235257063),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1099145895),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=848597656),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=748452672),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1837113970),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1136453204),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=310873524),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=703092490),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=524928034),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2049865179),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1424846968),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1615755351),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1387195172),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1975066708),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1890056369),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=802240808),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1670313329),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1709001499),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1347249354),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1889548106),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=298939263),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2051597823),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=496633784),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1260900009),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1030346489),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=410763571),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=240703280),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2106791656),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1090417180),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1963162725),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=53835231),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=14602488),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=919222678),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=185783238),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1450833141),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=414007416),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1698312065),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1020511265),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1285480326),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=437759819),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=166607105),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=855427914),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=769674665),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1583384024),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1002211629),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=669317730),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1553056014),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1740370380),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=34590699),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=811610221),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1540449917),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1174946563),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1072350804),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1068600663),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1071463302),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=964999524),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=695489717),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=535387614),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1706781148),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=2124308971),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=281238276),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1575126470),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1203405516),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=787678680),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=854625373),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=23412163),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1657014060),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=353159990),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=926689899),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1036101842),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=622220558),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1178966616),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=555135680),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=690394732),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=1318729262),\n",
       " DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                        random_state=527479103)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a378cd93-9e4d-4efc-95fb-58eb989f9c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_features='auto',\n",
       "                       random_state=884351046)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.estimators_[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe67ba-b32c-46a9-b131-c3c8540f1cc1",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "The Given the class imbalance ratio, Confusion matrix and accuracy is not meaningful\n",
    "for unbalanced classification. A robust evaluation is required to measure the\n",
    "performance of a fraud detection model.\n",
    "\n",
    "**1. False Positives:**\n",
    "A false positive is an outcome where the model incorrectly predicts the positive class.\n",
    "<br>**2. False Negatives:**\n",
    "A false negative is an outcome where the model incorrectly predicts the negative class.\n",
    "<br>**3. Precision:**\n",
    "Precision talks about how precise/accurate the model is i.e. out of those predicted positives, how many of them are actual positive. Precision is a good measure to determine, when the costs of False Positives is high. For instance, here, a false positive means that a transaction is that is non- fraudulent has been identified as fraudulent. This can happen if the precision is not high for the fraud detection model.\n",
    "<br>**4. Recall:**\n",
    "Recall calculates how many of the Actual Positives our model captures through labeling it as Positive (True Positive). If a fraudulent transaction is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.\n",
    "<br>**5. F1 Score:**\n",
    "F1 Score is used to seek a balance between Precision and Recall.\n",
    "<br>**6. Mathews Correlation Coefficient:**\n",
    "The coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1.<br> \n",
    "A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation.<br>\n",
    "The Matthews correlation coefficient is more informative than F1 score and\n",
    "accuracy in evaluating binary classification problems, because it takes into\n",
    "account the balance ratios of the four confusion matrix categories (true\n",
    "positives, true negatives, false positives, and false negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e58a2-7b4c-4e18-9792-3fd681151c30",
   "metadata": {},
   "source": [
    "### Predict and Evaluate by entring the values <b>model</b>, <b>x_test</b>, <b>y_test</b>\n",
    "\n",
    "<b><i>  '''Predict values for given model & test dataset\n",
    "    and evaluate the results in terms of FP, FN, F1-score,\n",
    "    Brier Score, AUC and G-Mean'''</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20e40715-c6b1-4b51-99dc-8ac71df67178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(model, x_test, y_test):\n",
    "\n",
    "    \n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions) * 100\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    mcc = matthews_corrcoef(y_test,predictions)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    \n",
    "    metrics = [fp, fn, round(accuracy,2), round(precision,2), round(recall,2), round(f1,2), round(mcc,2)]\n",
    "    table_row = [[model.__class__.__name__] + metrics]\n",
    "    display(HTML(tabulate.tabulate(table_row,headers=('Algorithm','False Positives', \n",
    "                                                  'False Negatives','Accuracy', 'Precision', \n",
    "                                                  'Recall', 'F1 Score', 'MCC'), \n",
    "                                   tablefmt='html')))\n",
    "    return [model.__class__.__name__] + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed65b7ae-5a81-44dc-a6a5-a1dc4b649f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm             </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>RandomForestClassifier</td><td style=\"text-align: right;\">             1249</td><td style=\"text-align: right;\">               12</td><td style=\"text-align: right;\">     97.79</td><td style=\"text-align: right;\">       0.06</td><td style=\"text-align: right;\">    0.88</td><td style=\"text-align: right;\">      0.12</td><td style=\"text-align: right;\"> 0.23</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_res = predict_and_evaluate(rf_clf, x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f197e-e5ad-43c5-ac2a-943d091dab13",
   "metadata": {},
   "source": [
    "#### Feature Importances\n",
    "\n",
    "In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dc78a8a-24ad-4525-ab8c-09ae1b23ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00556492, 0.01640873, 0.01136474, 0.01229732, 0.04808027,\n",
       "       0.10405239, 0.01064119, 0.01418718, 0.02504915, 0.01181636,\n",
       "       0.021856  , 0.0925133 , 0.09811093, 0.11228158, 0.01036249,\n",
       "       0.16991557, 0.00619014, 0.02858137, 0.08088703, 0.01651499,\n",
       "       0.0161331 , 0.01210541, 0.02202181, 0.00768574, 0.00731399,\n",
       "       0.0063937 , 0.00797839, 0.00889485, 0.00703009, 0.00776728])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2862ed6-a64a-4c10-bbec-c6ab4c1106a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(rf_clf.feature_importances_, index = x_train.columns)\n",
    "feature_importance.sort_values(ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b05fb1e-ff61-4fcb-a23f-d77be2cc24bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V14              0.169916\n",
       "V12              0.112282\n",
       "V4               0.104052\n",
       "V11              0.098111\n",
       "V10              0.092513\n",
       "V17              0.080887\n",
       "V3               0.048080\n",
       "V16              0.028581\n",
       "V7               0.025049\n",
       "V21              0.022022\n",
       "V9               0.021856\n",
       "V18              0.016515\n",
       "scaled_amount    0.016409\n",
       "V19              0.016133\n",
       "V6               0.014187\n",
       "V2               0.012297\n",
       "V20              0.012105\n",
       "V8               0.011816\n",
       "V1               0.011365\n",
       "V5               0.010641\n",
       "V13              0.010362\n",
       "V26              0.008895\n",
       "V25              0.007978\n",
       "V28              0.007767\n",
       "V22              0.007686\n",
       "V23              0.007314\n",
       "V27              0.007030\n",
       "V24              0.006394\n",
       "V15              0.006190\n",
       "scaled_time      0.005565\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d120daaf-d816-4c8f-a5c5-11a265abbb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAHeCAYAAAB5btiPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQOElEQVR4nO3dd5gkVbn48e+7yIJklaRIEhQEFAFFwcAioqBcLioCxosZBRWRq6AiggG8KEm8eEVJKggY+KEoKKIYQDIIgkpa8oJITkvY9/fHqWGLZmZ2qsP2dM/38zz1TNepqtNvddXUvHP61KnITCRJkqRhNa3fAUiSJEm9ZMIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitpyoqIHSMiI2KVfsei+S8iZlTHf0a/Y5HUWya80hRSS/BGm/bv0XtuHBFfjIilelH/VBYRi1Sf7Yx+x6JmWn4XXzXK8oiIG6vlv2hZVv+9fSwi7oyICyPikIhYa5S6VqnW3b2X+yRNZk/rdwCS+uILwHUtZZf36L02BvYGjgbu7tF7tOv7wI+A2f0OpE2LUD5bgN/3MY5B9Qfg6cAjfYzhYeAdwJ9ayjcBnsvY5+ZvgGOBAJYE1gX+C/hoRHwmMw/sTbjSYDLhlaamX2XmBf0OohMRsWhmPtBJHZn5OPB4l0KabyJiGjC933EMusycQ0k4++mXwNsi4uOZ+Vit/B3AhcDSY2z3z8z8Qb0gIvYAfg58IyL+npm/7EnE0gCyS4Okp4iILSPijxHxQETcFxGnRsTaLeu8OCKOjohrI+LhiJgVEUdGxLNq63wROKCava72Newqta9Zdxzl/bPa9ol6qrK1IuK4iLiLWotYRLyr+kr3oerr3R9FxIoT2M+n9OGNiJkR8Yuqf+cFVZ2XjXQbiIi3VPMPV++5XkudR0fE/RHxvIg4vfoMb4mIL0REtKy7aER8o/rqenZE/CMidh9lvYyIwyLinRHxN0qr307Av6pV9q59tl+c6PFp+WxXr9a/OyLuiYijImKRUT6zd0XEeRHxYETcFRF/iIjXt6wzkfNn+eo9bqr2/daI+H8xj/7UEfH7iPj9KOVHR8TMlrIdqmN0X0TcWx23T9SWP6UPb1X/5dW59rtqP2+OiE+P8p4rR8Qp1X7eHhEHRcQbWuuch+OBZwGb1+qdDmwLHDfBOgDIzH8DOwCPAZ9rsq007GzhlaamJSPiSS1HmXkHQES8GzgGOB34DOVr848Af4qI9TJzZrXJ5sDzgKOAWcDawIeAtSPiFZmZwE+BFwBvBz4J3FFt+y9gmTbiPgm4Cvgs5atcIuJzwJeAE4HvVvV+DPhDFe/dbbzP6pRk4/+AHwC7Az+PiJ2ArwL/W623J3BiRKxRtRaOWAA4DfgL8GlgC2AfyjX3C1XcAZwCbAp8D7gEeAPlH4QVKJ9X3WuB7YDDKJ/jpZTjcjjwM8pnDfDX6udEjk/diZRuLnsC6wMfAG6nnANUMe8NfBE4u9qPR4CXV7H9ulpnoufPT6qYvgnMBJatYl6pmu9IRGxOSSZ/W9uHFwKvBA6Zx+bPoBy/n1I+l22Br0XEZZn5q6r+RYEzgWdX9c2itMpu2jDUmcA5lN+RX1VlW1K6KfwI+HiTyjLzhog4C9g0IpbIzHsbxiMNp8x0cnKaIhOwI5CjTdXyxYC7gO+0bLccpf/td2plTx+l/h2q+l5dK9u9KlulZd1VqvIdR6kngS/W5r9YlR3Xst7KlNasz7aUrwM82lo+zuexSq1sZlW2Ua3s9VXZg8BKtfIPVeUzamVHV2WH1soC+AWlZXbpquw/q/U+1xLTScAcYLWWz+NxYK2WdZdu/azaOD4jn+33Wtb9KXBHbX71KoafAtNa1o0m5w+wVPWeu7dxDv8e+P0o5UcDM2vzBwP3AAuMU9eMUY7f76uyd9fKpgO3Aj+ule1WrfeftbKFgStb65zHufdSYGfg3pFjRkmyz6ydj78Y5ffjsHHqPrha58Utv2uNP28np2GZ7NIgTU07U1rT6hPVz6WA4yNi6ZGJkuicS631KjMfGnkdEQtX6/2lKlq/R3F/u2X+LZSuWSe2xDuL0hLctLVtxBWZeU5t/tzq55mZecMo5c8bpY7DRl5kZlbz04HXVcVvpHyuh7Zs9w1KgrxlS/lZmXnFRHegjePT+tn+EXhWRCxRzW9D+az3zSe3Zo/sH0z8/HmI0jo8IyKeMdF9auhuYFFqXQUauJ/Ssg9AZj4CnMeTj/MWwM2UVvqR9R4Gjmjj/U6k3Dy3VUQsDmxFw+4MLe6vfi7eQR3SULFLgzQ1nZej37T2/OrnmWNs98TXoxHxTMoIATtQvo6uW7LjCEfXOrLE8ynJ4VVjrP9om+9TT2rJzHuqbrU3tqx3T/WzNWmbA1zbUvbP6ucq1c+VgVsy876W9a6sLa9r3fdxtXF8bmiZv6v6+QzKcV+Nsl/jJd0TOn8yc3ZEfIaS3N8WEX+htIAfm5mzxqm/if+ldAH5VUTcTOlycWJmnjaBbW+qJfEj7gJeXJtfGbhmlPWubhpoZv4rIs6gdIlYhNIl5sdN66lZrPrZem5JU5YJr6S6kW993k1pJW1Vv4v8RMqQYwdQ+p/eX21/GhO7IbY1UQAgIhYYZ5uHWuanVfVsyeijLdw/StlEjDVyw1jlMUZ5N7Xu+7w0PT7d2LcJnz+ZeXBE/JzScvwGSj/sPSPitZl58TjvkWPE9KTzJjNvj4iXVHVvWU3vjYhjM/O/5rEf/TjOx1Fah5enjKJydwd1rUPZh0b/JEnDzIRXUt011c/bM/OMsVaqvobeDNg7M/etlT9/lNVHTWyZ24K4VEt5a8vmeK6hJCHXZeY/57XyfDSN8vV3PaYXVD9nVj+vB14XEYu3tPKuWVs+L2P909Dk+EzUNZT9WouSQI+1Dszj/BmRmddQWnm/UcV2CfAp4F3jbHYXo3checp5U3VF+DnlhsNplFbfD0fElzKzcUtsi+uBtSIiWlp5V2+zvp9RbpJ8BbB9u0FFxEqUMXzPGeXbA2nKsg+vpLrTKV87fzYiFmxdGBEjIyuMtIC1tnjtOkqdI2PlLlUvzHL3+B3Aa1rW/+jEw+WnVSx7V6Me1GONaBmCaz7bpR5LNf8oZdQAKOOvLlBfr/JJSiL7K+btwernUi3lTY7PRJ1M6dLwhSp5fELts5/Q+RPlCXELtyy+hvIV/ELziOMaYM3auUhErEsZfaH+Xk869lW/45ERLOb1HhNxOmU0ja1r77kw8MF2KsvM+ymjWXyRkqQ3VnVjOZ5yXn2lnTqkYWULr6QnZOa9EfERyhPILoqIH1GGEFsJeBPwZ2CXar0/AJ+uEpubKSMZrDpKtRdWP79S1fco8PMsD434LrBHRHwXuICS/L5glDrGiveaiPg8sB+wSkScTEmaVgXeDHwH+HqTz6BLHga2iIhjKDdrbUn5/L6amSNj5/4c+B3lc1mFMszY6ymjNxxctX6OKzMfiogrgO0j4p/AncDlmXl5g+MzIZl5dUR8BdgL+GNE/JQy6sTLgFuAPSd6/lCO8W8j4kRKn+DHKMdrOcpQXOM5kjJCwukR8T1K/+SdgL8BS9TW+26VAJ4J3ERpAf4YpRX5Sjr3f9W+HB8Rh1BGcXgncx9kMdY3G2PKzGMarP6CiHgX5Z+aJShPWnsbpf/ubhPsqyxNGSa8kp4kM4+LiFuAPYD/prSG3Uy5a/+o2qrvoIyhujPlj+6vKYndLS31nR8Re1GSki0o3yytSmn53Zcybu62VDcYVXXc3iDe/atk75PMfczujVU8p4y5YW89TtnXwyl9aO+jjMP7RPeCzJwTEVtXZdsD76V0d/hvytf8E/UBynE4iDIKxD6Ux0RP6Pg0kZlfiIjrKInjVygtzH+lJLgj60zk/LmR0hK5GaW/72PA34HtMvMn84jhyoh4D+VzO5CSML+72t8ZtVV/QBk27qOUFvBZwAmUIdyeNMpEOzLz/oh4LeUz/gSlj/SxlDGKf0Lvn+A2MrrKHEqr+nWU8Y+/02Q0D2mqiHzKDaaSpHZFxNHAtpm52LzW1fCJiF0p/3w8NzNv7nM4kir24ZUkqQ0R8fSW+YWBDwNXmexKk4tdGiRJas9PI+IGSr/gJSmjS6xJ6csraRIx4ZUkqT2nU/pQv5MyMsIVwA6ZeUJfo5L0FPbhlSRJ0lCzD68kSZKGml0aRlENov4cfA65JEnSZLY4cEvOo8uCCe/onkMZqFySJEmT23Mp432PyYR3dPcB3HjjjSyxxBLzWleSJEnz2b333suKK64IE/hG3oR3HEsssYQJryRJ0oDzpjVJkiQNNRNeSZIkDTUTXkmSJA01E15JkiQNNRNeSZIkDTUTXkmSJA01E15JkiQNNRNeSZIkDTUTXkmSJA01E15JkiQNNRNeSZIkDTUTXkmSJA21p/U7gEGyyh6nTnjdmfu/qYeRSJIkaaJs4ZUkSdJQM+GVJEnSUDPhlSRJ0lAz4ZUkSdJQM+GVJEnSUDPhlSRJ0lAz4ZUkSdJQM+GVJEnSUDPhlSRJ0lAz4ZUkSdJQM+GVJEnSUDPhlSRJ0lDre8IbETtHxMyIeDgizo2IDcdZd+2I+Em1fkbErmOst0JE/CAi/h0RD0XEZRHx0p7thCRJkiatvia8EbE9cCCwD7A+cClwekQsO8YmiwDXAnsAs8ao8xnAn4FHgS2BtYBPAXd1NXhJkiQNhKf1+f13A47IzKMAImIn4E3A+4D9W1fOzPOB86t1n7K88hngxsx8b63sum4GLUmSpMHRtxbeiJgObACcMVKWmXOq+Y06qHpr4IKIOCkibo+IiyPig/OIZaGIWGJkAhbv4P0lSZI0ifSzS8PSwALAbS3ltwHLd1Dv84CPAFcBbwAOBw6NiP8aZ5s9gXtq000dvL8kSZImkb7ftNYD04CLMvOzmXlxZn4HOALYaZxt9gOWrE3P7X2YkiRJmh/62Yf3DuBxYLmW8uUY44a0CboVuKKl7ErgrWNtkJmzgdkj8xHRwdtLkiRpMulbC29mPgJcCGw2UhYR06r5czqo+s/AGi1lLwCu76BOSZIkDah+j9JwIHBMRFwAnAfsCiwKjIzacCxwc2buWc1PpwwzBjAdWCEiXgLcn5lXV+UHAWdHxGeBE4ENgQ9VkyRJkqaYvia8mXlCRCwD7Eu5Ue0SYIvMHLmRbSVgTm2T5wAX1+Z3r6azgBlVnedHxJsp/XK/QBmSbNfM/GHv9kSSJEmTVb9beMnMw4DDxlg2o2V+JjDPDraZ+QvgF10IT5IkSQNuGEdpkCRJkp5gwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkoWbCK0mSpKFmwitJkqShZsIrSZKkodY44Y2IYyLiNb0IRpIkSeq2dlp4lwTOiIirIuKzEbFCt4OSJEmSuqVxwpuZ2wArAIcD2wMzI+JXEbFtRCzYThARsXNEzIyIhyPi3IjYcJx1146In1TrZ0TsOo+696jWO7id2CRJkjTY2urDm5n/yswDM3Nd4OXA1cD3gVsi4qCIeP5E64qI7YEDgX2A9YFLgdMjYtkxNlkEuBbYA5g1j7pfBnwY+OtE45EkSdJw6eimtYh4NrB5NT0O/BJ4EXBFRHxygtXsBhyRmUdl5hXATsCDwPtGWzkzz8/M/87MHwGzx4ltMeCHwAeBu+axHwtFxBIjE7D4BGOXJEnSJNfOTWsLRsRbI+IXwPXA24CDgedk5n9l5uuA7YAvTKCu6cAGwBkjZZk5p5rfqGlsLb4FnJqZZ8xzTdgTuKc23dThe0uSJGmSeFob29xKSZSPBzbMzEtGWed3wN0TqGtpYAHgtpby24A124gNgIjYgdI94mUT3GQ/SreKEYtj0itJkjQU2kl4PwmclJkPj7VCZt4NrNpuUJ2IiBWBQ4DNx4uxLjNnU+seERE9ik6SJEnzWzt9eDcFnjIaQ0QsGhFHNqzrDkrf3+VaypdjHjekjWMDYFngooh4LCIeAzYBPl7NL9BmvZIkSRpA7SS8/wU8fZTypwPvaVJRZj4CXAhsNlIWEdOq+XPaiA3gt5Qb515Smy6g3MD2ksx8vM16JUmSNIAm3KWhGr0gqmnxiKh3F1gAeCNwexsxHAgcExEXAOcBuwKLAkdV73sscHNm7lnNTwfWqradDqwQES8B7s/MqzPzPuDyltgfAP6dmU8qlyRJ0vBr0of3biCr6Z+jLE9g76YBZOYJEbEMsC+wPHAJsEVmjtzIthIwp7bJc4CLa/O7V9NZwIym7y9JkqTh1iTh3ZTSunsm8FbgztqyR4DrM/OWdoLIzMOAw8ZYNqNlfmYVR5P6Z8xzJUmSJA2lCSe8mXkWQESsCtyQmdmzqCRJkqQumVDCGxEvBi6vHgqxJPCisYbuykwf4ytJkqRJY6ItvJdQ+tfeXr1ORu9WkJQb2CRJkqRJYaIJ76rAv2qvJUmSpIEwoYQ3M68HiIgFKSMxfCkzr+tlYJIkSVI3NHrwRGY+ShmhQZIkSRoI7Txp7WRgm+6GIUmSJPVGk3F4R1wFfCEiXkl5LPAD9YWZeWg3ApMkSZK6oZ2E9/2Up65tUE11CZjwSpIkadJonPBmpqM0SJIkaWC004dXkiRJGhiNW3gj4sjxlmfm+9oPR5IkSequdvrwPqNlfkFgHWAp4MxOA5IkSZK6qZ0+vG9uLYuIacDhwDXdCEqSJEnqlq704c3MOcCBwCe7UZ8kSZLULd28aW012usiIUmSJPVMOzetHdhaBDwbeBNwTDeCkiRJkrqlnRbZ9Vrm5wD/Aj4FjDuCgyRJkjS/tXPT2qa9CESSJEnqhbb73EbEssAa1ew/MvP27oQkSZIkdU/jm9YiYomI+D5wC3BWNd0cET+IiCW7HaAkSZLUiXZGaTgCeDnlJrWlqmkr4KXA/3UrMEmSJKkb2unSsBXwhsz8U63s9Ij4IHBad8KSJEmSuqOdFt5/A/eMUn4PcFc7QUTEzhExMyIejohzI2LDcdZdOyJ+Uq2fEbHrKOvsGRHnR8R9EXF7RJwcEWuMUp0kSZKGXDsJ75eBAyNi+ZGC6vUBwJeaVhYR21Oe0rYPsD5wKaXFeNkxNlkEuBbYA5g1xjqbAN8CXgFsDiwI/DoiFm0anyRJkgZbO10aPgKsDtwQETdUZSsBs4FlIuLDIytm5voTqG834IjMPAogInai9A9+H7B/68qZeT5wfrXuU5ZX62xRn4+IHYHbgQ2AP7SuHxELAQvVihafQNySJEkaAO0kvCd3680jYjolCd1vpCwz50TEGcBG3XofYGT0iDvHWL4nsHcX30+SJEmTRDsPntini++/NLAAcFtL+W3Amt14g4iYBhwM/DkzLx9jtf0o3SpGLA7c1I33lyRJUn+1/eAJgIhYjJZ+wJl5b0cRdd+3gHWAV421QmbOpnTJACAi5kNYkiRJmh/aefDEqhFxakQ8wNyRGe4C7qb5KA13AI8Dy7WUL8fYN6RNWEQcRhlGbdPMtMVWkiRpCmqnhfcHQFBuKrsNyHbfPDMfiYgLgc2o+gZXXRA2Aw5rt94oTbTfBN4MzMjM69qtS5IkSYOtnYR3XWCDzPxHl2I4EDgmIi4AzgN2BRYFRkZtOBa4OTP3rOanA2tV204HVoiIlwD3Z+bVVfm3gHcA/wncVxtC7Z7MfKhLcUuSJGkAtJPwng+sCHQl4c3MEyJiGWBfYHngEmCLzBy5kW0lYE5tk+cAF9fmd6+ms4AZVdlHqp+/b3m79wJHdyNuSZIkDYZ2Et4PAN+OiBWAy4FH6wsz869NK8zMwxijC0NmzmiZn0npUjFefd51JkmSJKC9hHcZYDWqLgeVpCShSRlmTJIkSZoU2kl4j6R0KXg7Hd60JkmSJPVaOwnvysDWtRvEJEmSpEmr8Ti8wJmUkRokSZKkSa+dFt6fAwdFxIuAy3jqTWundCMwSZIkqRvaSXi/Xf38wijLvGmtoVX2OHVC683c/009jkSSJGk4NU54M7OdbhCSJElSX5i8SpIkaahNuIU3Ij4+kfUy89D2w5EkSZK6q0mXhk9OYJ0ETHglSZI0aUw44c3MVXsZiCRJktQL9uGVJEnSUDPhlSRJ0lAz4ZUkSdJQM+GVJEnSUDPhlSRJ0lBr59HCRMQ0YHVgWVqS5sz8QxfikiRJkrqiccIbEa8AjgNWBqJlcQILdCEuSZIkqSvaaeH9NnAB8CbgVkqSK0mSJE1K7SS8zwe2zcyrux2MumOVPU6d0Hoz939TjyORJEnqv3ZuWjuX0n9XkiRJmvTaaeH9JvCNiFgeuAx4tL4wM//ajcAkSZKkbmgn4f1J9fPIWllSbmDzpjVJkiRNKu10aVh1lOl5tZ+NRcTOETEzIh6OiHMjYsNx1l07In5SrZ8RsWundUqSJGl4NW7hzczruxlARGwPHAjsROkfvCtwekSskZm3j7LJIsC1wEnAQV2qU5IkSUNqQglvRGwN/CozH61ejykzT2kYw27AEZl5VPVeO1GGPHsfsP8o9Z8PnF+t+5Tl7dQZEQsBC9WKFm+4D5IkSZqkJtrCezKwPHB79XosjfrwRsR0YANgvycqyJwTEWcAG020ni7UuSewdzvvJ0mSpMltQn14M3PaSFeA6vVYU9Mb1pamJMi3tZTfRkmw29FOnfsBS9am57b53pIkSZpk2hmlYehk5mxg9sh8ROsTkyVJkjSo+p3w3gE8DizXUr4cMGsS1TmlTfTJbeDT2yRJ0uTTzrBkXZOZjwAXApuNlEXEtGr+nMlSpyRJkgZXv1t4oQwfdkxEXACcRxlCbFFgZISFY4GbM3PPan46sFa17XRghYh4CXB/Zl49kTolSZI0dfQ94c3MEyJiGWBfyk1llwBbZObITWcrAXNqmzwHuLg2v3s1nQXMmGCdkiRJmiLaSngjYjXgvcBqwCcy8/aI2BK4ITP/1rS+zDwMOGyMZTNa5mdSHmPcdp2SJEmaOhr34Y2ITYDLgJcDbwEWqxatC+zTvdAkSZKkzrVz09r+wOczc3PgkVr5mcAruhKVJEmS1CXtJLwvAn42SvntlIc+SJIkSZNGOwnv3cCzRylfD7i5o2gkSZKkLmsn4f0R8LWIWB5IYFpEvBL4OnBsN4OTJEmSOtXOKA2fBb4F3AgsAFxR/TwO+HL3QtMwm+jT23xymyRJ6lTjhLd6ktkHI2JfSn/exYCLM/OqbgcnSZIkdartB09k5o3AjRGxAPCiiHhGZt7VvdAkSZKkzrUzDu/BEfH+6vUClCecXURJfmd0NTpJkiSpQ+3ctLYtcGn1+j+A5wFrAgcBX+lSXJIkSVJXtJPwLg3Mql6/ETgxM/8JHEnp0ytJkiRNGu0kvLcBa1XdGbYAflOVLwI83q3AJEmSpG5o56a1o4ATgVsp4/CeUZW/HPh7l+KSJEmSuqKdYcm+GBGXAysCJ2Xm7GrR48D+3QxOkiRJ6lRbw5Jl5o9HKTum83AkSZKk7mor4Y2IRYFNgJWA6fVlmXloF+KSJEmSuqJxwhsR6wG/pNyktihwJ2XkhgeB2wETXkmSJE0a7YzScBDwc+AZwEPAK4CVgQuB3bsXmiRJktS5dhLelwDfyMw5lBvVFqoeM/xp4KtdjE2SJEnqWDsJ76PAnOr17ZR+vAD3UEZukCRJkiaNdm5auxh4GXAVcBawb0QsDbwbuLyLsUmSJEkda6eF97OUh04AfA64CzgcWAb4UJfikiRJkrqiccKbmRdk5u+q17dn5haZuURmbpCZl7YTRETsHBEzI+LhiDg3Ijacx/pvi4i/V+tfFhFvbFm+WEQcFhE3RcRDEXFFROzUTmySJEkabO208BIRT4uI10XEhyNi8arsORGxWBt1bQ8cCOwDrA9cCpweEcuOsf7GwPHA94D1gJOBkyNindpqBwJbAO8CXggcDBwWEVs3jU+SJEmDrXHCGxErA5cB/w/4FqUrA8BngK+3EcNuwBGZeVRmXgHsRBnT931jrP8J4LTMPCAzr8zMvYCLgF1q62wMHJOZv8/MmZn5HUoiPW7LsSRJkoZPOzetHQJcAKwL/LtW/jPgiCYVRcR0YANgv5GyzJwTEWcAG42x2UaUFty604FtavNnA1tHxJHALcAM4AXAJ8eIYyFgoVrR4hPeCU0aq+xx6oTWm7n/m3ociSRJmkzaSXhfDWycmY9ERL18JrBCw7qWBhYAbmspvw1Yc4xtlh9j/eVr8x8DvgPcBDxGGUbtg5n5hzHq3BPYe+JhS5IkaVC004d3GiVJbfVc4L7Owumaj1GeALc1pQX5U8C3IuJ1Y6y/H7BkbXru/AhSkiRJvddOC++vgV2ZOwRZVjer7QP8smFdd1Ce1rZcS/lywKwxtpk13voR8XTKE9/enJkj33H/NSJeQnn08RmtFWbmbGD2yHxLy7UkSZIGWDstvLsDr4yIK4CFgeOY253hM00qysxHgAuBzUbKImJaNX/OGJudU1+/snlt/QWraU7LOo/T5qgUkiRJGlyNW3gz88aIWBfYnnLj2mKUIcJ+mJkPtRHDgcAxEXEBcB6l9XhR4CiAiDgWuDkz96zWPwQ4KyI+BZwK7AC8lKrFOTPvjYizgAMi4iHgemAT4D2UESEkSZI0hTRKeCNiQeDvwFaZ+UPgh50GkJknRMQywL6UG88uAbbIzJEb01ai1lqbmWdHxDuAL1O6LlwFbJOZ9cca70Dpl/tD4JmUpPdzwLc7jVeSJEmDpVHCm5mPRsTC3Q4iMw8DDhtj2YxRyk4CThqnvlnAe7sVnyRJkgZXO31avwV8JiLaueFNkiRJmq/aSVpfRrlp7PURcRnwQH1hZr6lG4FJkiRJ3dBOwns38JMuxyFJkiT1RDujNNg3VpIkSQOjcR/eiFg1Ip4/SvnzI2KVrkQlSZIkdUk7N60dDWw8SvnLq2WSJEnSpNFOwrse8OdRyv8CvKSjaCRJkqQuayfhTWDxUcqXBBboLBxJkiSpu9pJeP8A7BkRTyS31es9gT91KzBJkiSpG9oZluwzlKT3HxHxx6rs1cASwGu7FZgkSZLUDY1beDPzCuDFwInAspTuDccCa2bm5d0NT5IkSepMW48HzsxbgM92ORZJkiSp69rpw0tEvDoifhARZ0fEClXZuyPiVd0NT5IkSepMOw+eeCtwOvAQsD6wULVoSWz1lSRJ0iTTTgvv54GdMvODwKO18j9TEmBJkiRp0mgn4V2DMkpDq3uApTqKRpIkSeqydhLeWcDqo5S/Cri2s3AkSZKk7mon4T0COCQiXk556tpzIuKdwNeBw7sZnCRJktSpdoYl25+SKP8WWITSvWE28PXM/GYXY5MkSZI61jjhzcwEvhIRB1C6NiwGXJGZ93c7OEmSJKlTbT14IiKC8ijh26onr0mSJEmTUqM+vBGxfEQcC9wF3AbcHhF3RcSREbFcTyKUJEmSOjDhFt6IWAI4m9KF4Sjg70AAawFvB14VEevbtUGSJEmTSZMW3k8AjwNrZ+YnM/P/MvPbmflxYG1K8vvxdoKIiJ0jYmZEPBwR50bEhvNY/20R8fdq/csi4o2jrPPCiDglIu6JiAci4vyIWKmd+CRJkjS4miS8bwK+mpn/al2QmbcD+wH/0TSAiNgeOBDYh/KktkuB0yNi2THW3xg4HvgesB5wMnByRKxTW2c14E+UVugZwIuBLwEPN41PkiRJg61JwvsCSpeGsZxNeQpbU7sBR2TmUdUNcDsBDwLvG2P9TwCnZeYBmXllZu4FXATsUlvnK8AvM/PTmXlxZl6TmadUibkkSZKmkCYJ7xLA3eMsv7taZ8IiYjqwAXDGSFlmzqnmNxpjs43q61dOH1k/IqZRWqP/GRGnR8TtVTeJbcaJY6GIWGJkAhZvsh+SJEmavJokvAHMGWd5Vus0sTSwAGXEh7rbgOXH2Gb5eay/LOXGuj2A04DXAz8DfhoRm4xR557APbXpponvgiRJkiazJuPwBqXVNMdZPhmMJPH/LzMPql5fUvX93Qk4a5Rt9qP0Ix6xOCa9kiRJQ6FJwvveHrz/HZSRH1rH8F0OmDXGNrPmsf4dwGNA6wMxrgReNVqFmTmb8nhkAMpzNSRJkjQMJpzwZuYx3X7zzHwkIi4ENqOMtjDSB3cz4LAxNjunWn5wrWzzqnykzvN56g10LwCu71bskiRJGgxtPVq4yw4EjomIC4DzgF2BRSkPt6B6stvNmblntf4hwFkR8SngVGAH4KXAh2p1HgCcEBF/AH4HbEEZMm1Gr3dGkiRJk0vfE97MPCEilgH2pdx4dgmwRWaO3Ji2ErWb5TLz7Ih4B/Bl4KvAVcA2mXl5bZ2fRcROlJvRDgX+Abw1M/80H3ZJkiRJk0jfE16AzDyMMbowZOaMUcpOAk6aR51HAkd2Iz5JkiQNribDkkmSJEkDx4RXkiRJQ61xl4aIWADYkTJSwrK0JM2Z+dquRCZJkiR1QTt9eA+hJLynApdTnrAmSZIkTUrtJLw7ANtl5i+7HYwkSZLUbe304X0EuLrbgUiSJEm90E7C+w3gE+HzdyVJkjQA2unS8CpgU2DLiPgb8Gh9YWa+pRuBSZIkSd3QTsJ7N/CzLschSZIk9UTjhDcz39uLQCRJkqRe8METkiRJGmrtdGkgIrYFtgNWAqbXl2Xm+l2IS5IkSeqKxi28EfFx4CjgNmA94Dzg38DzgF91NTpJkiSpQ+208H4U+FBmHh8ROwL/k5nXRsS+wDO7Gp3UR6vsceqE1525/5t6GIkkSepEO314VwLOrl4/BCxevf4+8PZuBCVJkiR1SzsJ7yzmtuTeALyier0q4MMoJEmSNKm0k/CeCWxdvT4KOCgifgOcgOPzSpIkaZJppw/vh6gS5cz8VkT8G9gYOAX4vy7GJkmSJHWsnQdPzAHm1OZ/BPyom0FJkiRJ3dLWgyci4tUR8YOIOCciVqjK3h0Rr+pueJIkSVJn2hmH963A6ZQRGtYDFqoWLQl8tnuhSZIkSZ1rp4X388BOmflB4NFa+Z8Bn7ImSZKkSaWdhHcN4A+jlN8DLNVRNJIkSVKXtTsO7+qjlL8KuLadICJi54iYGREPR8S5EbHhPNZ/W0T8vVr/soh44zjrfjsiMiJ2bSc2SZIkDbZ2Et4jgEMi4uVAAs+JiHcCXwcOb1pZRGwPHAjsQ+kScSlwekQsO8b6GwPHA9+j9CE+GTg5ItYZZd03Ux6McUvTuCRJkjQc2kl49weOA34LLEbp3vBd4P8y85tt1LcbcERmHpWZVwA7AQ8C7xtj/U8Ap2XmAZl5ZWbuBVwE7FJfqRo94pvAO3lyX2NJkiRNIY0T3iy+Qnm88DqUFtRlqsSzkYiYDmwAnFGrf041v9EYm21UX79yen39iJgGfB84IDP/NoE4FoqIJUYmYPFGOyJJkqRJq50nrQGQmY8AV3T4/ksDCwC3tZTfBqw5xjbLj7H+8rX5zwCPAYdOMI49gb0nuK4kSZIGyIQT3og4ciLrZeZYXRHmi4jYgNLtYf3MzAluth+lH/GIxYGbuh2bJEmS5r8mLbw7AtcDFwPRpfe/A3gcWK6lfDnKaBCjmTWP9V8NLAvcEPFEmAsA34iIXTNzldYKM3M2MHtkvradJEmSBlyThPdw4O3AqsBRwA8y885O3jwzH4mIC4HNKKMtjPS/3Qw4bIzNzqmWH1wr27wqh9J3d7Q+vt+v4pYkSdIUMuGb1jJzZ+DZwP8A/wHcGBEnRsQborMm0QOBD0bEf0XECymJ9aJUyWlEHBsR+9XWPwTYIiI+FRFrRsQXgZdSJciZ+e/MvLw+UUZpmJWZ/+ggTkmSJA2gRjetVV/9Hw8cHxErU7o5/C/wtIhYOzPvbxpAZp4QEcsA+1JuPLsE2CIzR25MWwmYU1v/7Ih4B/Bl4KvAVcA2VWIrSZIkPUnbozRQktCk9OddoJMgMvMwxujCkJkzRik7CTipQf2rtBubJEmSBlujcXir8WrfHhG/Af4JvIjywIeV2mndlSRJknqtybBk/wvsANwIHAm8PTPv6FVgkiRJUjc06dKwE3ADcC2wCbDJaPeqZeZbuhOaJEmS1LkmCe+xlD67kiRJ0sCYcMKbmTv2MA5JkiSpJxrdtCZJkiQNGhNeSZIkDTUTXkmSJA01E15JkiQNNRNeSZIkDTUTXkmSJA01E15JkiQNtSYPnpDUoVX2OHVC683c/009jkSSpKnDFl5JkiQNNRNeSZIkDTUTXkmSJA01E15JkiQNNRNeSZIkDTVHaZAGnCM/SJI0Plt4JUmSNNRMeCVJkjTUTHglSZI01Ex4JUmSNNQmxU1rEbEz8N/A8sClwMcy87xx1n8b8CVgFeAq4DOZ+ctq2YLAl4E3As8D7gHOAPbIzFt6uBvSUJjoTXAw8RvhvLFOktRPfW/hjYjtgQOBfYD1KQnv6RGx7BjrbwwcD3wPWA84GTg5ItapVlmkqudL1c+3AGsAp/RuLyRJkjRZ9T3hBXYDjsjMozLzCmAn4EHgfWOs/wngtMw8IDOvzMy9gIuAXQAy857M3DwzT8zMf2TmX6plG0TESr3fHUmSJE0mfU14I2I6sAGlywEAmTmnmt9ojM02qq9fOX2c9QGWBBK4e4w4FoqIJUYmYPEJ7YAkSZImvX638C4NLADc1lJ+G6U/72iWb7J+RCwMfA04PjPvHaPOPSl9fUemm+YZuSRJkgZCvxPenqpuYDsRCOAj46y6H6UVeGR6bu+jkyRJ0vzQ71Ea7gAeB5ZrKV8OmDXGNrMmsn4t2V0ZeO04rbtk5mxgdm3bicQuSZKkAdDXFt7MfAS4ENhspCwiplXz54yx2Tn19Sub19evJbvPB16Xmf/uYtiSJEkaIP1u4YUyJNkxEXEBcB6wK7AocBRARBwL3JyZe1brHwKcFRGfAk4FdgBeCnyoWn9B4MeUIcm2AhaIiJH+vXdWSbYkSZKmiL4nvJl5QkQsA+xLufHsEmCLzBy5MW0lYE5t/bMj4h2Uh0t8lfLgiW0y8/JqlRWAravXl7S83abA77u/F5IkSZqs+p7wAmTmYcBhYyybMUrZScBJY6w/k3KTmqQh1u2nt/mEOUkaXkM9SoMkSZJkwitJkqShNim6NEiSJsZuEpLUnC28kiRJGmomvJIkSRpqJrySJEkaavbhlaQpbFCGY7PvsqROmPBKkqacXiT6kiYvE15JkrrAVmhp8jLhlSRpkjKJlrrDm9YkSZI01GzhlSRpirDvsqYqW3glSZI01GzhlSRJbRuEYehs2ZYJryRJUkPeUDhYTHglSZImAZPo3jHhlSRJGkKD8iTF+cGEV5IkSX0zP5JoR2mQJEnSUDPhlSRJ0lAz4ZUkSdJQM+GVJEnSUDPhlSRJ0lCbFAlvROwcETMj4uGIODciNpzH+m+LiL9X618WEW9sWR4RsW9E3BoRD0XEGRHx/N7uhSRJkiajvie8EbE9cCCwD7A+cClwekQsO8b6GwPHA98D1gNOBk6OiHVqq30a+DiwE/By4IGqzoV7tBuSJEmapCbDOLy7AUdk5lEAEbET8CbgfcD+o6z/CeC0zDygmt8rIjYHdgF2iogAdgW+nJn/r6rzPcBtwDbAj1orjIiFgIVqRYsD3HvvvU9ab87sBye8U63bjmWidU60vl7U6X7P/zrd7/lfp/s9/+t0v+d/ne73/K/T/e5dnU3eIzJzwit3W0RMBx4Ets3Mk2vlxwBLZeZ/jrLNDcCBmXlwrWwfYJvMXDcingdcA6yXmZfU1jkLuCQzPzFKnV8E9u7SbkmSJGn+eW5m3jzeCv1u4V0aWIDS+lp3G7DmGNssP8b6y9eWM491Wu1H6VZR90zgzjHWH7E4cBPwXOC+eaw7UYNQ5yDE2Is6ByHGXtQ5CDH2os5BiLEXdQ5CjL2ocxBi7EWdgxBjL+ochBh7UecgxNi0zsWBW+ZVYb8T3kkhM2cDs1uK59lOXnpPAHBfZk68XX3A6xyEGHtR5yDE2Is6ByHGXtQ5CDH2os5BiLEXdQ5CjL2ocxBi7EWdgxBjL+ochBjbqHNC79nvm9buAB4HlmspXw6YNcY2s+ax/qxa2UTrlCRJ0pDqa8KbmY8AFwKbjZRFxLRq/pwxNjunvn5l89r611ES23qdS1BGaxirTkmSJA2pydCl4UDgmIi4ADiPMsLCosDIqA3HAjdn5p7V+ocAZ0XEp4BTgR2AlwIfAsjMjIiDgc9HxFWUBPhLlP4dJ3c59tmU4dRau0MMe52DEGMv6hyEGHtR5yDE2Is6ByHGXtQ5CDH2os5BiLEXdQ5CjL2ocxBi7EWdgxBjT+rs6ygNTwQRsQvw35Sbyi4BPp6Z51bLfg/MzMwda+u/DfgysApwFfDpzPxlbXlQPqgPAUsBfwI+mpn/7PnOSJIkaVKZFAmvJEmS1Cv9vmlNkiRJ6ikTXkmSJA01E15JkiQNNRNeSZIkDTUTXkmSJA01E94ORcSOEbFkv+MYT0TsHRFL9zuOVhGxZESsUU2T+jMcERHLRcRKXarr+RGxWUSs3ub2k+6YjiUiFo2I10TE9hHxtojYIGrPjmyzzgVa5jeMiFdExEId1HltRDxrlPKlIuLaNutcKSJeHhEvG63udkXE0yJi84h4f0S8rvXzmAwiYqFOjketngUi4nnVg4lG6t0uInaIiNananas+mw7/j2PiAW7EU9LndHNY119lqt14zj1UnXtXb7fcQyDiJgREU/vdxxjiYjVI+INIzF2+rfiCZnp1MEEPAK8sM1t3wh8F/gfYM2WZc8AzmxY3xKjTEtWMW44Utbl/V8XeLzhNh8ArqA8Vro+XQG8v40Y7gO+B2zcxf1aHPgBcD1wDDAd+BYwp4r1rCafJbAnsFnt2J5R1TVS36+ApRrG+DjwW+AdwELdPK7jvOcLgWsbrD+tOr8fqB3nkf2eCfxHGzGsDFwAPFZ9bksAv6nVew3wgjb3bw6w7CjlywGzG9b10er8aT3P/wRs0EZs3wS2ql4/F7iy+gxmVT//CqzQRr0frc7HE0fO0dqypZsc72qbzYFfAnfV9vmuqux1bcT3YsqDgx4HLgNWrH7eX/3u3wm8rMvneaPrGrAdML02v0vt2N8BfKGNGJ5GGW/+LGCfquy/q9+l2SPXpYZ17ghsVL1emHLdfKyK81Hg202uJa2/K8BLqrj+DPwYmNHGfj+z2vYG4HBgAcrfyZFr5dnAs9s8pp+vzvelW5YtARzZsL4PVPv63mp+++p38tqR49WwvsuAvYAVu3kuj/FebeUtlDxigdr8VtX5eTPlmvyeDuN6FnP/Nj4OPK8qPxL4Rsf73esPdlim6qI62jQHuHtkvkF976guNL8A/gg8BLyztny5JhfcapvWP6z1BOOJn13+XNYF5jRYf+SCvR8wg5JAvbB6/VXKH7HdG8YwB7i8+nkl8ClgmQ7365tVXR8Dfkd5St9lwCuB1wB/A77SoL4bgfWq10cAFwHrUf7orEt57PV329jvX1H++N1ZxfySHv8eNE0E9qf8I7MV8Lrq4vhpYE1gX+Bh4PUNY/gx8PuqzhMoCeTvgBWAZwOnAT9rWOfW1TQHeHdtfmvgzcBhwD8a1Lc75Y/ALsz9B28vYAvg2Op34KUNY5wFrFO9PoGS5C9dzT8T+DlwUsM6P17Fchjw/epc2rO2vNF1CPgvSuJ0PCW52rKadgSOo/yhfXfDGE8DTgLWAQ6uPssTgQUpSeH3gd/0+Tx/nCr5A95LuZ7vQ2nU+BzluvaBhjF8qTrm36Bcbw6nJIHvBN4D3ER56FKTOq8FXl69PoDyJNI3V7+P/wn8A/ifNvd74+r4/p7yT+6vq3PhNQ1j/B7lWrtLVdfJwKWUa+9GlCeyHtOwztdX5/bllH9E7gA27eA837U6pj+h/DP2uarOzwFfAO4BPtQwxjlVHY9V5/xbgad1eB5fNMY0p/o9ugi4qM3j/R/V/DGUfyKOqI73mzuI99hq359L+Wd2JOF9A/C3Tj6LTBPeJgfiPkpy+l+1acfq5PzsSFmD+i6mPFFuZH676hfo/dV8OwnvTVWMmwKbVNOMKsYdR8oa1vnTeUy/bXihuB7Ybpzl2wM3NIxxDrAs5Y/UN4F/Vxe3n1D+2EYbx/uGkQsi8JzqPbaqLX8T8PcG9T0MrFy9vo6WPwLABsAtbe730pQk/2/VBehC4CO00ZpPedT3eNP3Gx7vW4BX1+ZXqH6XFqrm9wLObhjj7VSJPeUbjDnAq2rL1wdmtfFZ1v8xrE+zKYnAVg3quw7Ysjb/Asofs6dV84cAv24Y40PAqtXrG4ENW5avA/yrYZ1/A95Rm9+4+nz3reabJgL/BHYeZ/lHgasaxngnVWsU8HTK9WzD2vK1gTsa1jlWIjAyXdlwv5/4ZgA4F/jvluUfoUFiUW1zDXNb9Fevzs3ta8u3Ay5rWOfDwErV638AW7Qsfw1wfZv7/Wvgey3LDwZ+2zDGW6i+ravOvznA5rXlrwRualjn2VQNFEBQ/um+b2T/2zjPrxz5vaE0XDxK7dtJ4P3ABQ1jnEP5W7MNcEpV5+3A12n/W+RHKY0ie9emL1bn0rdGyto83n8E9mtZ/lngnHZirbafBaxbva4nvM8D7m+33ifq77SCqTJVF5zzKP/NLNZyQq3VRn33U/3xqpVtWh3knZr+AlbbPxP4GXAmta82242xtu0vgaPGmP5fwwvFQ+P98gJrAQ82jPFJX0MDCwFvp3w18jglOdi3YZ0PU/tqidIK9oLa/MrAAw3q+wfwpur1tbR0v6B8FXhPJ/tdlW1EaSG5t4r52IZ1jiTMvxtjOr/h8b535KJVzU+rzqnla8d7wp9jrc5VW+pbt7Z8deDeNs/362j5urPNeh4AVqnNRxXns6v5dYH7GtZ5KVXCQ2mdeV3L8o2Afzes88F6nFXZOpQ/PPs1vQ5VvzdrjLN8DeChhjHeBTy/er0gJeFdv7Z8TRp8u1aL82ienAjUp2833O85VN8qAf+qn49V2WpNz0nKtXLFlvk1a/OrtlHnTOb+I38TLd8yUL5tm3BiwZMToFuAV7QsX5vm/4Q9QNU4UM0/QvXNRm2/GyU/lBbX1VrK3kH5O7xVG+f5g1T/ONTOp7Vr86sDdzWMsfXv2LMpXeH+ydyuHO9rWOcrgasp3zZMq5W3m7fUj/dttHTNqn6/G+13y/b31X7X6wnvS2l4bRu1/k4rmEoT5euzr1Un0Cs7PHGecnGoyjepDvSXm/wCttTxEcrXqW/vJMZq278yTr9aSqLW5ELxB8o/DU/5qobSV+sY4KyGMT7xNcsoy1ahfDXYtNX4Zp78R/W4lovR2jTrwrI7JUlZHditunitVi1blZJMNv06erz9XpTSyvDnhnX+A3hXF4/3n4HP1eZ3qF8QKclV02TlHOBL1ev3UiVnteV70bB1pdsT5RucD9bmX0v5Qz7yOPc1aJ6s7Ej5520GpdvFFcBmlFahTavf1SMa1nkDtRb4Wvla1ed6TMPjfSHjfCVOuX5e2DDGMyh9OFegfF18FbX+lpSWqj80rPMC4CPjLG96nte7wtxI1U+2tnxtmv9DOwt4UW3+zzy5IWPNNur8SnXtWYryD80pVA04wCKUrjKnN9zv1Sh9YK+l6rZVW74azf+hvYTqWwLKN3T3ArvVlu9E85bt2xml33x1PXqgqrPJ8b6DWsNNdcxXrs2vTvN/aMe7ns+gfLvWuJWT8i3Y8cBfmPs3p5OEdwalX/1MWvrOU65rjfa7ZftfMvfafh/lb+M0ShemH7db7xP1d1rBVJwof7yup/Q5faTNE+dkxujYXp1Q9zf5BRyljrWqC8dx7Z7cVT1HAd8aZ/kLgesa1Pdi4NbqgvFTSr+0w6vXd1D+EVinYYyj3mjUsk6jbg2Ur4E+PM7yHWmeTB5anS9XUlprHqd8Xf44peV0+W7vdxvH+4fAQeMsX5dmfbY3o7R+nEvpv/sosGtt+e40/8rzDdXnN7v6+RpKon4uJRl+jHG6zUww5q9Skqwj61ODOrarjvUJlKTxPp6clH+Yhl05qu12o/yBfrB27oxMP6P27dME6zturONNSdJub3Idql27/krpAvOZajqQ0kJ9H837dL60ujbMqeJZm/LH+1bKP6YP0nKz3QTqPAQ4eJzlqwG/a1BfazeYz7Usfz/NuzScyTjd5IC30fxr8+mUb+XupHRBeKg6n/5ZHbfraXDDJ3O7AY10Bfpgy/Ktad6F5Z3V7/BV1bVj2+o4n0BJ3GYzTreZMer8NWPcG0L5NvCRhuf5n6h1Lxll+VY0T8on8nes7ZvOKY0DtwIfov28pbXb164ty3egg762lAaQ25h7b8pJlH/sZ9HSQt/ONNLaoIaq4YWOoLSsvCIz/9Fw+00oX2vvN8byTSl3PL63QZ3rZObltfnplJuGNgXekpnXNYmxqmMhyl2ZDzbddpw6FwfeBbwCGBlmZhYlWTkuM+9tWN/ewAFdjvHVwF8z854xlm9J+Wr29w3rfSHlYvg8yn+ut1Jabs7Ihr+MEbELpUVvdpPt5lHn8pT+tdd3qb51KPu5PaWryemZ+Zsu1LsKpd/zhZk5sxqaamdKK9Wpmfm7Nuvdm9KKeAHl2DzpmGTmmxvUtSXlPB/Z7yNqy55V1ffvBvWtk5mXR8RSlJtwRlo/bqX883XVROuq1fkiSsvX0WO9J/DWzNynQZ2rUL5lGu33+9uZObNhjOtQupqsSblx8P6IWJiSGD2dcsNa0+vvk66VnZpXfRGxFfBoZp7eoM4XVNuMet2OiHcAj2XmiW3EuwXlpqPW69BxmflAg3o2aSm6NTP/WVv+CcpIEgc0jO+VlPPnnMw8OyLWAvag/H7/PDOPaVjfmyn/aH1yjOXvoCTrmzaI74HMvGSM5R+ldCE4rEGMR1Hu67lvots0FRHPpzRqvJTSsHRFw+1Xbim6v34Ni4j3AGTmsR3EuCTlhsV1gcUofeq/lZm3tlvnE3Wb8A6PiJhDaSn8LvCjXv7iDLtB+CwHLMbvAcdPxhjrIuJWyp3v3+93LK16cbwH6Bw6j3IOdXO/u1ZnS33HZ+b9ncYo9UI1lvXilC5VUyoB9METXdKtgco7rG8Tyl3X3wBujYhjqpbKnunBfi/YzfqqOif9Z9nmfs/3492GkRi/Tonx6El+Tk6n9HHsqS6dk934LDehDNU02c+hK+hujN2us17frEG89mpqyMw5mXlPL5LdbpyTEbFwlIcIbRURW9enjgPstE+E0xN9T9ali2PcdlIf5Yal91L6TM6h9M/6DA37iA7afg/yZzmZYqTLDyKYn59jFz7LrwF7dTumyXy8u11nL86fQdjvXsXY7XOo28enR9eLSV/nIMTYqzq7fU7Wtt+C0k+/tU/8nE7qfaL+bu3oVJ86PdC9qo9yt+hXKHdjPwKcMhX2e9A+y8kSI11+EMH8/hw7/SwpNzPdRUlYvknLOMSTIcZef5ad1Dk/zp/JuN/zK8ZOz6FuH59eHO9BqHMQYuxVnd0+J1u2v4oy6spy3YqpPtmHd4Ii4qJ5rPJ0yt2tE3rGebfrm8d7LUq5wWM/yuNrJ1znIOz3IHyWgxBjte3IE+SOq+Y3powo8u3M/EJ1c9gtncY5mc7JlrrHu9ktM/O1E6xnII53t+ucX+dPJzHOzzon23ne7ePTi+M9CHUOQoy9qLPX17WIuJcytN017Ww/L0/rRaVDai3gR5S7hUfzbMrTlPpV31NExGuA91EeUTiH8pXG9xpWMwj7PQif5SDECOXO/yf6sGa5Q/q1wBkRsSDlyUn9jrFnn2VO8C7tCRiU493tOnt6/nQpxp7XOYnP824fn14c70GocxBi7EWdvb6u/ZgytGFPEt6uNxkP60T3Byrvan217Z5DebzfPykX2j9R+pQtOqz7PQif5SDEWNXX1QcRDMI52YtpUI53t+vsxfkzCPvdo/p6ca3s9oNGenG9mPR1DkKMPdrvnl57qYaVpDwF8VOULhlPTO3WOzLZwjtxf6Y8RWQs91GeItav+oiIXwGvowzSfixloPxG41OOYtLvdy/q7MFnOQgxQvkj/RbKc9KfkJlXRMRmlCfC9TvGXpxDwBNdGsbs55UT7NLAgBzvHtTZ1fOnRzF2vc4BOs+7fXy6frwHpM5BiLEXdfbs2lt5O2WM8YcpLb31a3FSHt7Uvk4z5qky0fDpX/O7vqrOU4D/pDwoYlLG2aP9nvSf5SDEWNX5ImDH8fYD2LvPMXb9s6zVfVDLdBjlj8bdwCFDeLy7fZ539fwZoP0eiPO8B7/fvTjek77OQYixR/vds2tvVf8syrck03pRvzetTVD0dqDySTngOwzGfg/CZzkIMcLUfRDBBN7zi5TH9u4+wfUH4nh32yCcP4Oih9fKrh2fXhzvQahzEGLsRZ29vq5FxJ3Ay7JHN6354ImJ24TeDVQ+WQd8h8HY70H4LAchRpi6DyKYlx9QbkKaqEE53t02COfPoOjVtbKbx6cXx3sQ6hyEGHtRZ6+va8dQHkPfG71snh7GiQEeqHzY93sQPstBiHFQ4pyfMQLvpgzfM2ljnEzTVN3vQfksp+r13P2evPtd1XsopftYT8ZB72jjqT4xYAOVT6X9HoTPchBiHJQ4uxUj8NOW6WfAX4DHaNh/bhA/x8l8bJwG41o5CDG635Nzvyk30Y01ndnpvtuHt0MxiQYqn58GYb8H4bMchBhhMOLsRowRcVRL0RzgX5SL7a8nQ4yDaKrudy8MwrVyEGLsRZ2DEGMv6hyY3+9OM+apOgGvoYwVdx9wD3AE8IrJUt9U3u9B+CwHIcZBidMYJ+80Vfd7UD7LqXo9d78n7373cup7AIM0MckHKp/K+z0In+UgxDgocfY6RmAD4F3VtN5kjHGyTlN1vwfls5yq13P3e3LuN6Xr2BK112NO7X4GT7xXpxVMlQn4FfAocCvwNWCNyVTfVN7vQfgsByHGQYmzlzECywJnVhfyO6tpDvBbYJnJEONknqbqfg/KZzlVr+fu96Te76OAxavXR1fzo06dvpdPWpu4R4FtgV9k5uOTsL5eGYT9HoTPchBihMGIs5cxfhNYHFg7M68EiIi1KMPlHEp5ElC/Y5zMpup+98IgXCsHIcZe1DkIMfaizq7HmJnvrb3esRt1jsWb1iSpEhH3AK/LzPNbyjcEfp2ZS/UlMEkachFxJvCWzLy7pXwJ4OSc+KPdR+WDJyRprmmUVoxWj+L1UpJ6aQYwfZTyhYGOH3BhlwZJmutM4JCIeHtm3gIQESsAB1H68UqSuigiXlybXSsilq/NLwBsAdzc8fvYpUGSiohYETgFWBu4sSpekfJ4zq0z86Z+xSZJwygi5gAjyWiMsspDwMcy88iO3seEV5LmiogAXgesWRVdmZln9DEkSRpaEbEyJdG9FtiQ8rCfEY8At3fjJjkTXkmSJA2EiDgV+EBm3tpkO/vwSlJNRLwM2JQyJu+TblTLzN36EpQkacRrgKc33ciEV5IqEfFZ4MvAP4DbmNuvjJbXkqQBYsIrSXN9AnhfZh7d70AkSd3juJKSNNcc4M/9DkKS1F0mvJI010HAzv0OQpLUXY7SIEmViJgGnAq8ALiClqeuZeZb+hGXJKmIiPuAdTPz2ibb2YdXkuY6lDJCw++Af+ONapI02XwVuLPpRrbwSlKlajnYITNP7XcskjTsImLria6bmad08l628ErSXHcC1/Q7CEmaIk5umU+e/HjheqvsAp28kTetSdJcXwT2iYhF+h2IJA27zJw2MgGvBy4BtgSWqqY3AhcBW3T6XnZpkKRKRFwMrEZpYZjJU29aW78PYUnS0IuIy4GdMvNPLeWvBr6TmS/spH67NEjSXCf3OwBJmqJWA+4epfweYJVOK7eFV5IkSX0VEX8AHgbenZm3VWXLAccCC2fmJh3Vb8IrSZKkfoqI1YGfUcZBv7EqXhG4CtgmM6/uqH4TXkkqImIB4JPAdsBKwPT68sx8Zj/ikqSpICIC2BxYsyq6Ejgju5CsmvBKUiUi9gU+AHwD+DLwFUrfsW2AfTPz0L4FJ0lTREQsDMzuRqI7wmHJJGmudwIfzMxvAI8Bx2fmB4B9gVf0NTJJGmIRMS0i9oqIm4H7gVWr8i9FxPs7rd+EV5LmWh64rHp9P7Bk9foXwJv6EpEkTQ2fB3YEPg08Uiu/nPLNW0dMeCVprpuAZ1evr6EMhA7wMmB2XyKSpKnhPcCHMvOHwOO18kuZ26e3bSa8kjTXz4DNqtffBL4UEVdRhsU5sm9RSdLwWwEYbSSGacCCnVbugyckqZKZe9RenxAR1wMbA1dl5s/7F5kkDb0rgFcD17eUbwtc3GnlJrySNIbM/Avwl9byiDgV+EBm3jr/o5KkobQvcExErEBp1X1LRKxB6eqwVaeVOyyZJDUUEfcB62bmtf2ORZKGRUS8GvgCsC6wGHARZUjIX3dctwmvJDVjwitJg8Wb1iRJkjTU7MMrSZKk+S4i7gIm1NWg00e7m/BKkiSpH3adX29kwitJkqT5LjOPmV/vZcIrSc19Fbiz30FI0jCKiIWB6fWyzLy3ozodpUHSVBYRW0903cw8pZexSNJUFRGLAl8DtgOe1bo8MxfopH5beCVNdSe3zCcQLfMjOrrgSpLG9D/ApsBHgO8DO1MeN/xhYI9xtpsQhyWTNKVl5rSRCXg9cAmwJbBUNb2RMvj5Fn0KUZKmgv8APpqZPwEeA/6YmV8GPgu8s9PKbeGVpLkOBnbKzD/Vyk6PiAeB7wAv7EtUkjT8ngmMPMzn3moe4E/A4Z1WbguvJM21GnD3KOX3AKvM10gkaWq5Fli1ev13Sl9eKC2/d3dauQmvJM11PnBgRCw3UlC9PgA4r29RSdLwOwpYt3q9P7BzRDwMHES5BnfEURokqRIRqwM/A14A3FgVrwhcBWyTmVf3KzZJmkoiYmVgA+DqzPxrx/WZ8ErSXBERwObAmlXRlcAZ6cVSkgaWCa8kjaIa+Hy2ia4k9V5EHEppzT20pXwXYPXM3LWT+u3DK0mViJgWEXtFxM3A/VQ3UETElyLi/f2NTpKG2luBP49SfjawbaeVm/BK0lyfB3YEPg08Uiu/HPhAPwKSpCniWZQRcVrdCyzdaeUmvJI013uAD2XmD4HHa+WXMrdPrySp+65m9Af8bMnc8Xnb5oMnJGmuFSgX3VbTgAXncyySNJUcCBwWEcsAZ1ZlmwG7A5/otHITXkma6wrg1cD1LeXbAhfP/3AkaWrIzCMjYiHgc8BeVfF1lKdfHttp/Sa8kjTXvsAxEbECpVX3LRGxBqWrw1Z9jUyShlhEPB04JjMPr1p5l6MMEXlbV+p3xB1JmisiXg18gfLEn8WAi4B9M/PXfQ1MkoZYRPwa+GlmfjsilqI8XvhRyg1ru2Xm4R3Vb8IrSZKkfoqIO4BNMvNvEfEB4GPAepThyvbNzBd2Ur+jNEiSJKnfFgHuq16/ntLaOwf4C7Byp5Xbh1fSlBYRdwET+qorM5/Z43Akaaq6GtgmIn4GvAE4qCpfljIWb0dMeCVNdbv2OwBJEvsCx1ES3d9m5jlV+evpwig59uGVJElS30XE8sCzgUur7gxExIbAvZn5947qNuGVpKeKiIWB6fWyzOz4azVJ0vznTWuSVImIRSPisIi4HXgAuKtlkiQNIBNeSZrrf4DXAh8BZgMfAPYGbqE8fEKSNIDs0iBJlYi4AXhPZv4+Iu4F1s/MqyPi3cDbM/ONfQ5RktQGW3glaa5nAtdWr++t5gH+BLymLxFJkjpmwitJc10LrFq9/juwXfX6P4C7+xGQJKlzJrySNNdRwLrV6/2BnSPiYcq4kAf0LSpJUkfswytJY4iIlYENgKsz86/9jkeS1B4TXkmSJA01uzRIUiUiDo2Ij49SvktEHNyHkCRJXWDCK0lzvRX48yjlZwPbzudYJEldYsIrSXM9C7hnlPJ7gaXncyySpC4x4ZWkua4GthilfEvmjs8rSRowT+t3AJI0iRwIHBYRywBnVmWbAbsDn+hbVJKkjjhKgyTVRMRHgM8Bz6mKrgP2ycxj+xeVJKkTJrySVImIp1Ouiw9WrbzLAZsDV2Tm6f2NTpLULvvwStJc/w94T/X6UeAMYDfg5KrlV5I0gEx4JWmu9YE/Vq+3BW4DVqYkwU8Zn1eSNBhMeCVprkWA+6rXrwd+mplzgL9QEl9J0gAy4ZWkua4GtomIFYE3AL+uypeljMUrSRpAJrySNNe+wNeBmcC5mXlOVf564OJ+BSVJ6oyjNEhSTUQsDzwbuLTqzkBEbAjcm5l/72twkqS2mPBKkiRpqNmlQZIkSUPNhFeSJElDzYRXkiRJQ82EV5IkSUPNhFeSJElDzYRXkvokIo6OiBxlWr0Lde8YEXd3IUxJGnhP63cAkjTFnQa8t6XsX/0IZCwRsWBmPtrvOCSpXbbwSlJ/zc7MWS3T4xHxnxFxUUQ8HBHXRsTeEfFEI0VE7BYRl0XEAxFxY0T8b0QsVi2bARwFLFlrNf5itSwjYpt6ABFxd0TsWL1epVpn+4g4KyIeBt5ZLftARFxZxfT3iPhorY7pEXFYRNxaLb8+Ivbs5QcnSRNlC68kTTIR8WrgWODjwB+B1YDvVIv3qX7OqZZfBzwP+F/gf4CPAmcDu1IelbxGtf79DcPYH/gU5ZHKD0fEO6v6dqnK1gOOiIgHMvOYKpatge2AG4AVq0mS+s6EV5L6a6uIqCejvwKeAexfJZIA10bEXpSEdh+AzDy4ts3MiPg88G3go5n5SETcU1bLWW3GdXBm/nRkJiL2AT5VK7suItYCPgwcA6wEXAX8KcsjPK9v830lqetMeCWpv34HfKQ2/wDwV+CVEfG5WvkCwMIRsUhmPhgRrwP2BNYElqBcz59Y3oW4Lhh5ERGLUlqZvxcRR9TWeRpwT/X6aOA3wD8i4jTgF5n56y7EIUkdM+GVpP56IDOvrhdUfXH3Bn46yvoPR8QqwC+Aw4HPAXcCrwK+B0wHxkt4E4iWsgVHi6v2erHq5weBc1vWexwgMy+KiFWBLYHXASdGxBmZue04sUjSfGHCK0mTz0XAGq2J8IiI2IBy0/GnMnNOVbZdy2qPUFqFW/0LeHatrucDi4wXTGbeFhG3AM/LzB+Os969wAnACRHxY+C0iHhmZt45Xv2S1GsmvJI0+ewL/CIibgB+TLlBbV1gncz8PHA1pVX2YxHxc+CVwE4tdcwEFouIzYBLgQerrg5nArtExDmUhPhrwESGHNsbOLTqG3wasBDwUuAZmXlgROwG3Eq5oW0O8DZgFnB3W5+AJHWRw5JJ0iSTmacDWwGvB84H/gJ8kupGsMy8FNgN+AxwOWXYsD1b6jibchPbCZRW3U9Xiz4F3EgZ/eE44OuM3wVipL7vAh+gjBl8GXAWsCNllAiA+6r3uKCKeRXgjSMt0JLUT1FuppUkSZKGky28kiRJGmomvJIkSRpqJrySJEkaaia8kiRJGmomvJIkSRpqJrySJEkaaia8kiRJGmomvJIkSRpqJrySJEkaaia8kiRJGmomvJIkSRpq/x/zOp08mkscpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,4), dpi=100)\n",
    "feature_importance.plot.bar()\n",
    "plt.title(\"Feature importances using MDI\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Mean Decrease in Impurity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd77c80-2ec8-476a-a11a-8579265746ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dd63185-1f39-47a3-87fa-5c32bc69e8d8",
   "metadata": {},
   "source": [
    "## **EXERCISE:** Train RF model again by considering only important features (e.g. top 10) and evaluate the model and observe the difference in the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3092218-ec67-4697-8fa1-71832f77be56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>1.034951</td>\n",
       "      <td>-0.296653</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>1.034963</td>\n",
       "      <td>0.038986</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.034975</td>\n",
       "      <td>0.641096</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>1.034975</td>\n",
       "      <td>-0.167680</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>1.035022</td>\n",
       "      <td>2.724796</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_time  scaled_amount         V1         V2        V3        V4  \\\n",
       "0         -0.994983       1.783274  -1.359807  -0.072781  2.536347  1.378155   \n",
       "1         -0.994983      -0.269825   1.191857   0.266151  0.166480  0.448154   \n",
       "2         -0.994972       4.983721  -1.358354  -1.340163  1.773209  0.379780   \n",
       "3         -0.994972       1.418291  -0.966272  -0.185226  1.792993 -0.863291   \n",
       "4         -0.994960       0.670579  -1.158233   0.877737  1.548718  0.403034   \n",
       "...             ...            ...        ...        ...       ...       ...   \n",
       "284802     1.034951      -0.296653 -11.881118  10.071785 -9.834783 -2.066656   \n",
       "284803     1.034963       0.038986  -0.732789  -0.055080  2.035030 -0.738589   \n",
       "284804     1.034975       0.641096   1.919565  -0.301254 -3.249640 -0.557828   \n",
       "284805     1.034975      -0.167680  -0.240440   0.530483  0.702510  0.689799   \n",
       "284806     1.035022       2.724796  -0.533413  -0.189733  0.703337 -0.506271   \n",
       "\n",
       "              V5        V6        V7        V8  ...       V20       V21  \\\n",
       "0      -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307   \n",
       "1       0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775   \n",
       "2      -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998   \n",
       "3      -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300   \n",
       "4      -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -5.364473 -2.606837 -4.918215  7.305334  ...  1.475829  0.213454   \n",
       "284803  0.868229  1.058415  0.024330  0.294869  ...  0.059616  0.214205   \n",
       "284804  2.630515  3.031260 -0.296827  0.708417  ...  0.001396  0.232045   \n",
       "284805 -0.377961  0.623708 -0.686180  0.679145  ...  0.127434  0.265245   \n",
       "284806 -0.012546 -0.649617  1.577006 -0.414650  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed5bedf0-df1c-4648-84ae-a772d24ce771",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [df['scaled_time'],df['scaled_amount'],df['V14'], df['V10'], df['V17'], df['V4'], df['V12'], df['V11'], df['V7'], df['V3'], df['V16'], df['V2'], df['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19938110-faf9-4e8d-95e0-d87efd8dc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"scaled_time\",\"scaled_amount\",\"V14\", \"V10\", \"V17\", \"V4\", \"V12\", \"V11\", \"V7\", \"V3\", \"V16\", \"V2\", \"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efaf1ef0-9d13-469d-920b-254523478371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat(data, axis=1, keys=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d30259e9-bf33-4940-b8ea-790e1a7a0b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V14</th>\n",
       "      <th>V10</th>\n",
       "      <th>V17</th>\n",
       "      <th>V4</th>\n",
       "      <th>V12</th>\n",
       "      <th>V11</th>\n",
       "      <th>V7</th>\n",
       "      <th>V3</th>\n",
       "      <th>V16</th>\n",
       "      <th>V2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>1.034951</td>\n",
       "      <td>-0.296653</td>\n",
       "      <td>4.626942</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>2.711941</td>\n",
       "      <td>-1.593105</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>1.107641</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>1.034963</td>\n",
       "      <td>0.038986</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>-0.150189</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.711757</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.034975</td>\n",
       "      <td>0.641096</td>\n",
       "      <td>-0.510602</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>0.313502</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>0.063119</td>\n",
       "      <td>0.411614</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>0.140716</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>1.034975</td>\n",
       "      <td>-0.167680</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.962886</td>\n",
       "      <td>-1.933849</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>1.035022</td>\n",
       "      <td>2.724796</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.031513</td>\n",
       "      <td>-1.040458</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_time  scaled_amount       V14       V10       V17        V4  \\\n",
       "0         -0.994983       1.783274 -0.311169  0.090794  0.207971  1.378155   \n",
       "1         -0.994983      -0.269825 -0.143772 -0.166974 -0.114805  0.448154   \n",
       "2         -0.994972       4.983721 -0.165946  0.207643  1.109969  0.379780   \n",
       "3         -0.994972       1.418291 -0.287924 -0.054952 -0.684093 -0.863291   \n",
       "4         -0.994960       0.670579 -1.119670  0.753074 -0.237033  0.403034   \n",
       "...             ...            ...       ...       ...       ...       ...   \n",
       "284802     1.034951      -0.296653  4.626942  4.356170  1.991691 -2.066656   \n",
       "284803     1.034963       0.038986 -0.675143 -0.975926 -0.025693 -0.738589   \n",
       "284804     1.034975       0.641096 -0.510602 -0.484782  0.313502 -0.557828   \n",
       "284805     1.034975      -0.167680  0.449624 -0.399126  0.509928  0.689799   \n",
       "284806     1.035022       2.724796 -0.084316 -0.915427 -0.660377 -0.506271   \n",
       "\n",
       "             V12       V11        V7        V3       V16         V2  Class  \n",
       "0      -0.617801 -0.551600  0.239599  2.536347 -0.470401  -0.072781      0  \n",
       "1       1.065235  1.612727 -0.078803  0.166480  0.463917   0.266151      0  \n",
       "2       0.066084  0.624501  0.791461  1.773209 -2.890083  -1.340163      0  \n",
       "3       0.178228 -0.226487  0.237609  1.792993 -1.059647  -0.185226      0  \n",
       "4       0.538196 -0.822843  0.592941  1.548718 -0.451449   0.877737      0  \n",
       "...          ...       ...       ...       ...       ...        ...    ...  \n",
       "284802  2.711941 -1.593105 -4.918215 -9.834783  1.107641  10.071785      0  \n",
       "284803  0.915802 -0.150189  0.024330  2.035030 -0.711757  -0.055080      0  \n",
       "284804  0.063119  0.411614 -0.296827 -3.249640  0.140716  -0.301254      0  \n",
       "284805 -0.962886 -1.933849 -0.686180  0.702510 -0.608577   0.530483      0  \n",
       "284806 -0.031513 -1.040458  1.577006  0.703337 -0.302620  -0.189733      0  \n",
       "\n",
       "[284807 rows x 13 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62e8deac-8ae1-4a36-86fb-cbbf4e8df9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 13)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ba4f1ee-cc13-4697-95c1-630de5ea4d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Fraud :  99.83 % of the dataset\n",
      "Frauds :  0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Non Fraud : \", round(df3['Class'].value_counts()[0]/len(df3) * 100,2), '% of the dataset')\n",
    "print(\"Frauds : \", round(df3['Class'].value_counts()[1]/len(df3) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e88d4b67-a4d7-487f-ba59-c5af5d515ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Fraud  284315  are normal transaction\n",
      "Frauds  492  are fraud\n"
     ]
    }
   ],
   "source": [
    "print(\"Non Fraud \", round(df3['Class'].value_counts()[0],2), ' are normal transaction')\n",
    "print(\"Frauds \", round(df3['Class'].value_counts()[1],2), ' are fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0d123b7-5a61-4343-badb-8712315088c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits = 1,\n",
    "                           test_size = 0.2,\n",
    "                           train_size = 0.8,\n",
    "                           random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd59522d-f4ba-424c-9350-766961b1a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df3.drop('Class', axis = 1)\n",
    "y1 = df3['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dc28c7b-cf5a-4c87-b2ae-d74ca745e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in ss.split(x1, y1):\n",
    "    train_df1 = df3.iloc[train_index]\n",
    "    test_df1 = df3.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf59ba81-f382-4f42-a2ad-e840380c0dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V14</th>\n",
       "      <th>V10</th>\n",
       "      <th>V17</th>\n",
       "      <th>V4</th>\n",
       "      <th>V12</th>\n",
       "      <th>V11</th>\n",
       "      <th>V7</th>\n",
       "      <th>V3</th>\n",
       "      <th>V16</th>\n",
       "      <th>V2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221234</th>\n",
       "      <td>0.678979</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>0.569714</td>\n",
       "      <td>-0.048368</td>\n",
       "      <td>-0.237470</td>\n",
       "      <td>0.276477</td>\n",
       "      <td>-0.086572</td>\n",
       "      <td>-1.433705</td>\n",
       "      <td>0.445171</td>\n",
       "      <td>-1.997102</td>\n",
       "      <td>-0.620654</td>\n",
       "      <td>-0.010040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53575</th>\n",
       "      <td>-0.453964</td>\n",
       "      <td>-0.135401</td>\n",
       "      <td>-1.375245</td>\n",
       "      <td>-0.657439</td>\n",
       "      <td>1.223388</td>\n",
       "      <td>0.966039</td>\n",
       "      <td>1.170186</td>\n",
       "      <td>1.762968</td>\n",
       "      <td>-0.236608</td>\n",
       "      <td>0.212730</td>\n",
       "      <td>-0.063234</td>\n",
       "      <td>0.177618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163269</th>\n",
       "      <td>0.365535</td>\n",
       "      <td>-0.125900</td>\n",
       "      <td>-0.742340</td>\n",
       "      <td>-0.233874</td>\n",
       "      <td>-0.452111</td>\n",
       "      <td>-2.647515</td>\n",
       "      <td>-0.653155</td>\n",
       "      <td>-1.092282</td>\n",
       "      <td>0.654774</td>\n",
       "      <td>1.400824</td>\n",
       "      <td>1.372909</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246069</th>\n",
       "      <td>0.802770</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>1.196803</td>\n",
       "      <td>-0.928533</td>\n",
       "      <td>0.754119</td>\n",
       "      <td>-1.269844</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>-0.595004</td>\n",
       "      <td>-1.164202</td>\n",
       "      <td>-0.745326</td>\n",
       "      <td>1.450172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198872</th>\n",
       "      <td>0.563905</td>\n",
       "      <td>-0.200796</td>\n",
       "      <td>-0.047681</td>\n",
       "      <td>-0.302465</td>\n",
       "      <td>-0.632465</td>\n",
       "      <td>-1.007567</td>\n",
       "      <td>0.737696</td>\n",
       "      <td>-1.670722</td>\n",
       "      <td>0.852613</td>\n",
       "      <td>-0.246758</td>\n",
       "      <td>0.091765</td>\n",
       "      <td>1.615882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>-0.621283</td>\n",
       "      <td>-0.296793</td>\n",
       "      <td>-0.056859</td>\n",
       "      <td>-0.806754</td>\n",
       "      <td>-0.052083</td>\n",
       "      <td>-0.019310</td>\n",
       "      <td>0.160261</td>\n",
       "      <td>-0.943760</td>\n",
       "      <td>-0.837459</td>\n",
       "      <td>0.791631</td>\n",
       "      <td>-0.137833</td>\n",
       "      <td>0.793163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226406</th>\n",
       "      <td>0.703967</td>\n",
       "      <td>1.087543</td>\n",
       "      <td>-0.994458</td>\n",
       "      <td>1.372831</td>\n",
       "      <td>0.771743</td>\n",
       "      <td>-2.283739</td>\n",
       "      <td>0.083809</td>\n",
       "      <td>-0.384317</td>\n",
       "      <td>-0.092902</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>-1.261566</td>\n",
       "      <td>-0.305330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23564</th>\n",
       "      <td>-0.609382</td>\n",
       "      <td>-0.257249</td>\n",
       "      <td>0.243658</td>\n",
       "      <td>-0.337445</td>\n",
       "      <td>0.618390</td>\n",
       "      <td>0.571020</td>\n",
       "      <td>0.452742</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.366709</td>\n",
       "      <td>1.754365</td>\n",
       "      <td>-0.893923</td>\n",
       "      <td>0.752513</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88109</th>\n",
       "      <td>-0.266709</td>\n",
       "      <td>2.040103</td>\n",
       "      <td>-0.116470</td>\n",
       "      <td>0.820355</td>\n",
       "      <td>-0.505563</td>\n",
       "      <td>0.648536</td>\n",
       "      <td>0.513435</td>\n",
       "      <td>0.461294</td>\n",
       "      <td>-0.844338</td>\n",
       "      <td>0.974853</td>\n",
       "      <td>-0.878728</td>\n",
       "      <td>-1.034119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42982</th>\n",
       "      <td>-0.509522</td>\n",
       "      <td>-0.167819</td>\n",
       "      <td>-0.400196</td>\n",
       "      <td>-0.211347</td>\n",
       "      <td>-0.146061</td>\n",
       "      <td>1.419232</td>\n",
       "      <td>0.752800</td>\n",
       "      <td>-0.445240</td>\n",
       "      <td>-0.530623</td>\n",
       "      <td>1.430730</td>\n",
       "      <td>-0.032176</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227845 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_time  scaled_amount       V14       V10       V17        V4  \\\n",
       "221234     0.678979      -0.293440  0.569714 -0.048368 -0.237470  0.276477   \n",
       "53575     -0.453964      -0.135401 -1.375245 -0.657439  1.223388  0.966039   \n",
       "163269     0.365535      -0.125900 -0.742340 -0.233874 -0.452111 -2.647515   \n",
       "246069     0.802770       0.009642  1.196803 -0.928533  0.754119 -1.269844   \n",
       "198872     0.563905      -0.200796 -0.047681 -0.302465 -0.632465 -1.007567   \n",
       "...             ...            ...       ...       ...       ...       ...   \n",
       "21691     -0.621283      -0.296793 -0.056859 -0.806754 -0.052083 -0.019310   \n",
       "226406     0.703967       1.087543 -0.994458  1.372831  0.771743 -2.283739   \n",
       "23564     -0.609382      -0.257249  0.243658 -0.337445  0.618390  0.571020   \n",
       "88109     -0.266709       2.040103 -0.116470  0.820355 -0.505563  0.648536   \n",
       "42982     -0.509522      -0.167819 -0.400196 -0.211347 -0.146061  1.419232   \n",
       "\n",
       "             V12       V11        V7        V3       V16        V2  Class  \n",
       "221234 -0.086572 -1.433705  0.445171 -1.997102 -0.620654 -0.010040      0  \n",
       "53575   1.170186  1.762968 -0.236608  0.212730 -0.063234  0.177618      0  \n",
       "163269 -0.653155 -1.092282  0.654774  1.400824  1.372909  0.135593      0  \n",
       "246069  0.779412  0.175939 -0.595004 -1.164202 -0.745326  1.450172      0  \n",
       "198872  0.737696 -1.670722  0.852613 -0.246758  0.091765  1.615882      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "21691   0.160261 -0.943760 -0.837459  0.791631 -0.137833  0.793163      0  \n",
       "226406  0.083809 -0.384317 -0.092902  0.001853 -1.261566 -0.305330      0  \n",
       "23564   0.452742  0.488449  0.366709  1.754365 -0.893923  0.752513      0  \n",
       "88109   0.513435  0.461294 -0.844338  0.974853 -0.878728 -1.034119      0  \n",
       "42982   0.752800 -0.445240 -0.530623  1.430730 -0.032176 -0.001649      0  \n",
       "\n",
       "[227845 rows x 13 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6c8c9e3-1355-49ac-b4f3-580c04098845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions: \n",
      "\n",
      "Train Set\n",
      "0    227451\n",
      "1       394\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Test Set\n",
      "0    56864\n",
      "1       98\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Percentage:\n",
      "\n",
      "Train Set\n",
      "0    99.827075\n",
      "1     0.172925\n",
      "Name: Class, dtype: float64\n",
      "\n",
      "Test Set\n",
      "0    99.827955\n",
      "1     0.172045\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distributions: \\n')\n",
    "print(\"Train Set\")\n",
    "print(train_df1.Class.value_counts())\n",
    "print(\"\\nTest Set\")\n",
    "print(test_df1.Class.value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(\"\\nTrain Set\")\n",
    "print((train_df1.Class.value_counts()/ len(train_df1))*100)\n",
    "print(\"\\nTest Set\")\n",
    "print((test_df1.Class.value_counts()/ len(test_df1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bac79647-1e7b-4dc4-ad33-84987851c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = train_df1.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3de1d53-40dd-4015-9b72-b4efbfb25346",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df1 = train_df1.loc[train_df1['Class'] == 1]\n",
    "non_fraud_df1 = train_df1.loc[train_df1['Class'] == 0][:394]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "604f30fd-ab9c-480e-ae44-11c40dd6b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df1 = pd.concat([fraud_df1, non_fraud_df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0b3d4c0-ff1e-4c83-a145-19135b00fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = normal_df1.sample(frac = 1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf1b0a7e-2db0-4566-976a-a330e7b603bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V14</th>\n",
       "      <th>V10</th>\n",
       "      <th>V17</th>\n",
       "      <th>V4</th>\n",
       "      <th>V12</th>\n",
       "      <th>V11</th>\n",
       "      <th>V7</th>\n",
       "      <th>V3</th>\n",
       "      <th>V16</th>\n",
       "      <th>V2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18598</th>\n",
       "      <td>-0.646883</td>\n",
       "      <td>-0.097953</td>\n",
       "      <td>-0.052868</td>\n",
       "      <td>-0.781729</td>\n",
       "      <td>0.463724</td>\n",
       "      <td>0.083425</td>\n",
       "      <td>0.099693</td>\n",
       "      <td>-0.160871</td>\n",
       "      <td>0.264224</td>\n",
       "      <td>0.880171</td>\n",
       "      <td>0.220499</td>\n",
       "      <td>1.210993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151008</th>\n",
       "      <td>0.113606</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-3.926207</td>\n",
       "      <td>-22.187089</td>\n",
       "      <td>-12.462315</td>\n",
       "      <td>8.904157</td>\n",
       "      <td>-10.592305</td>\n",
       "      <td>4.419997</td>\n",
       "      <td>-31.197329</td>\n",
       "      <td>-30.177317</td>\n",
       "      <td>-6.809890</td>\n",
       "      <td>16.497472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118308</th>\n",
       "      <td>-0.113476</td>\n",
       "      <td>0.113743</td>\n",
       "      <td>-0.660968</td>\n",
       "      <td>-1.039638</td>\n",
       "      <td>0.278142</td>\n",
       "      <td>0.317131</td>\n",
       "      <td>-0.664684</td>\n",
       "      <td>-0.395608</td>\n",
       "      <td>1.078234</td>\n",
       "      <td>0.645789</td>\n",
       "      <td>0.530852</td>\n",
       "      <td>0.985633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20198</th>\n",
       "      <td>-0.632526</td>\n",
       "      <td>1.157130</td>\n",
       "      <td>-0.913787</td>\n",
       "      <td>3.245086</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>2.861292</td>\n",
       "      <td>-0.677096</td>\n",
       "      <td>0.675288</td>\n",
       "      <td>1.767760</td>\n",
       "      <td>1.199930</td>\n",
       "      <td>-1.024502</td>\n",
       "      <td>0.885657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43428</th>\n",
       "      <td>-0.507372</td>\n",
       "      <td>4.781527</td>\n",
       "      <td>-9.373859</td>\n",
       "      <td>-14.110184</td>\n",
       "      <td>-19.236292</td>\n",
       "      <td>9.505594</td>\n",
       "      <td>-10.834006</td>\n",
       "      <td>5.299236</td>\n",
       "      <td>-16.701694</td>\n",
       "      <td>-18.649853</td>\n",
       "      <td>-9.899247</td>\n",
       "      <td>8.584972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70589</th>\n",
       "      <td>-0.361318</td>\n",
       "      <td>3.351778</td>\n",
       "      <td>-4.058523</td>\n",
       "      <td>-5.459602</td>\n",
       "      <td>-5.603400</td>\n",
       "      <td>-0.572676</td>\n",
       "      <td>-2.330271</td>\n",
       "      <td>2.378537</td>\n",
       "      <td>-3.039520</td>\n",
       "      <td>-2.372652</td>\n",
       "      <td>-2.522661</td>\n",
       "      <td>1.573578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181746</th>\n",
       "      <td>0.474829</td>\n",
       "      <td>0.809055</td>\n",
       "      <td>-0.226130</td>\n",
       "      <td>0.871966</td>\n",
       "      <td>-0.054060</td>\n",
       "      <td>-0.929527</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.882925</td>\n",
       "      <td>-1.083305</td>\n",
       "      <td>-0.649090</td>\n",
       "      <td>1.327337</td>\n",
       "      <td>-1.264126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274475</th>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.250122</td>\n",
       "      <td>-3.565119</td>\n",
       "      <td>-2.516628</td>\n",
       "      <td>-1.731413</td>\n",
       "      <td>1.970759</td>\n",
       "      <td>-2.513104</td>\n",
       "      <td>0.874052</td>\n",
       "      <td>-0.055178</td>\n",
       "      <td>-3.171195</td>\n",
       "      <td>-2.015713</td>\n",
       "      <td>2.361594</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>-0.838461</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-18.049998</td>\n",
       "      <td>-13.136698</td>\n",
       "      <td>-14.744902</td>\n",
       "      <td>8.594342</td>\n",
       "      <td>-17.131301</td>\n",
       "      <td>11.228470</td>\n",
       "      <td>-9.252794</td>\n",
       "      <td>-12.752811</td>\n",
       "      <td>-9.723565</td>\n",
       "      <td>8.287421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153823</th>\n",
       "      <td>0.182462</td>\n",
       "      <td>-0.275554</td>\n",
       "      <td>-7.383370</td>\n",
       "      <td>-18.913243</td>\n",
       "      <td>-13.806568</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>-12.686308</td>\n",
       "      <td>4.971249</td>\n",
       "      <td>-28.011293</td>\n",
       "      <td>-22.801238</td>\n",
       "      <td>-8.582309</td>\n",
       "      <td>14.706335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_time  scaled_amount        V14        V10        V17  \\\n",
       "18598     -0.646883      -0.097953  -0.052868  -0.781729   0.463724   \n",
       "151008     0.113606      -0.293440  -3.926207 -22.187089 -12.462315   \n",
       "118308    -0.113476       0.113743  -0.660968  -1.039638   0.278142   \n",
       "20198     -0.632526       1.157130  -0.913787   3.245086   0.022045   \n",
       "43428     -0.507372       4.781527  -9.373859 -14.110184 -19.236292   \n",
       "70589     -0.361318       3.351778  -4.058523  -5.459602  -5.603400   \n",
       "181746     0.474829       0.809055  -0.226130   0.871966  -0.054060   \n",
       "274475     0.955556       0.250122  -3.565119  -2.516628  -1.731413   \n",
       "9252      -0.838461      -0.293440 -18.049998 -13.136698 -14.744902   \n",
       "153823     0.182462      -0.275554  -7.383370 -18.913243 -13.806568   \n",
       "\n",
       "               V4        V12        V11         V7         V3       V16  \\\n",
       "18598    0.083425   0.099693  -0.160871   0.264224   0.880171  0.220499   \n",
       "151008   8.904157 -10.592305   4.419997 -31.197329 -30.177317 -6.809890   \n",
       "118308   0.317131  -0.664684  -0.395608   1.078234   0.645789  0.530852   \n",
       "20198    2.861292  -0.677096   0.675288   1.767760   1.199930 -1.024502   \n",
       "43428    9.505594 -10.834006   5.299236 -16.701694 -18.649853 -9.899247   \n",
       "70589   -0.572676  -2.330271   2.378537  -3.039520  -2.372652 -2.522661   \n",
       "181746  -0.929527   0.050890   0.882925  -1.083305  -0.649090  1.327337   \n",
       "274475   1.970759  -2.513104   0.874052  -0.055178  -3.171195 -2.015713   \n",
       "9252     8.594342 -17.131301  11.228470  -9.252794 -12.752811 -9.723565   \n",
       "153823  12.114672 -12.686308   4.971249 -28.011293 -22.801238 -8.582309   \n",
       "\n",
       "               V2  Class  \n",
       "18598    1.210993      0  \n",
       "151008  16.497472      1  \n",
       "118308   0.985633      1  \n",
       "20198    0.885657      1  \n",
       "43428    8.584972      1  \n",
       "70589    1.573578      1  \n",
       "181746  -1.264126      0  \n",
       "274475   2.361594      1  \n",
       "9252     8.287421      1  \n",
       "153823  14.706335      1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc0a852f-bd45-45bd-af8e-9f78b6fe62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df4['Class'].value_counts()/len(df4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c432c08-7da4-4d27-bdfb-a6eed66e57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = df4.drop('Class', axis = 1)\n",
    "y_train1 = df4['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4cf9532-1807-4236-ac2b-5702bccdbedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = test_df1.drop('Class', axis = 1)\n",
    "y_test1 = test_df1['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c83bd9cc-3f56-453b-bc8f-9f66ed590482",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_1 = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9835e717-4f50-4050-a8a6-fbdb665e98e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_1.fit(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c726430-c9be-4cd6-a7a2-bb1ee96fe7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(model, x_test1, y_test1):\n",
    "\n",
    "    \n",
    "    predictions = model.predict(x_test1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test1, predictions) * 100\n",
    "    precision = precision_score(y_test1, predictions)\n",
    "    recall = recall_score(y_test1, predictions)\n",
    "    f1 = f1_score(y_test1, predictions)\n",
    "    mcc = matthews_corrcoef(y_test1,predictions)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test1, predictions).ravel()\n",
    "    \n",
    "    metrics = [fp, fn, round(accuracy,2), round(precision,2), round(recall,2), round(f1,2), round(mcc,2)]\n",
    "    table_row = [[model.__class__.__name__] + metrics]\n",
    "    display(HTML(tabulate.tabulate(table_row,headers=('Algorithm','False Positives', \n",
    "                                                  'False Negatives','Accuracy', 'Precision', \n",
    "                                                  'Recall', 'F1 Score', 'MCC'), \n",
    "                                   tablefmt='html')))\n",
    "    return [model.__class__.__name__] + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd88f587-9943-447d-8296-3a53f1904ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm             </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>RandomForestClassifier</td><td style=\"text-align: right;\">             1058</td><td style=\"text-align: right;\">               10</td><td style=\"text-align: right;\">     98.13</td><td style=\"text-align: right;\">       0.08</td><td style=\"text-align: right;\">     0.9</td><td style=\"text-align: right;\">      0.14</td><td style=\"text-align: right;\"> 0.26</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_res1 = predict_and_evaluate(rf_clf_1, x_test1, y_test1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f105840-f013-437f-874f-7b0cfa814d3a",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84d0d90e-45e8-4191-a86d-c18b44021052",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "gb_clf_1 = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "588573c2-e27d-44e3-9117-3de9f22d63da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf.fit(x_train, y_train)\n",
    "gb_clf_1.fit(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b15811da-1b09-40a2-8692-73ba7beb3eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm                 </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GradientBoostingClassifier</td><td style=\"text-align: right;\">             2048</td><td style=\"text-align: right;\">                7</td><td style=\"text-align: right;\">     96.39</td><td style=\"text-align: right;\">       0.04</td><td style=\"text-align: right;\">    0.93</td><td style=\"text-align: right;\">      0.08</td><td style=\"text-align: right;\"> 0.19</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gb_res = predict_and_evaluate(gb_clf, x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c416b5bc-8f97-432f-a546-1044659bbd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm                 </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GradientBoostingClassifier</td><td style=\"text-align: right;\">             1714</td><td style=\"text-align: right;\">                8</td><td style=\"text-align: right;\">     96.98</td><td style=\"text-align: right;\">       0.05</td><td style=\"text-align: right;\">    0.92</td><td style=\"text-align: right;\">      0.09</td><td style=\"text-align: right;\"> 0.21</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gb_res_1 = predict_and_evaluate(gb_clf_1, x_test1, y_test1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7df26e-beaa-447b-8c0d-c9b174330a3d",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32564e27-efa8-4f0d-bd14-a1aaab5f03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:19] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=10, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=10)\n",
    "xgb_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef3146b5-796d-4a33-9c6d-42d3ac405e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:29] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf_1 = XGBClassifier()\n",
    "xgb_clf_1.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59310d59-ee1e-4166-9240-ac426ba12db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm    </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>XGBClassifier</td><td style=\"text-align: right;\">             2600</td><td style=\"text-align: right;\">                9</td><td style=\"text-align: right;\">     95.42</td><td style=\"text-align: right;\">       0.03</td><td style=\"text-align: right;\">    0.91</td><td style=\"text-align: right;\">      0.06</td><td style=\"text-align: right;\"> 0.17</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_res = predict_and_evaluate(xgb_clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9190c323-0fba-4fd4-80b5-3d9c49c36acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm    </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>XGBClassifier</td><td style=\"text-align: right;\">             1705</td><td style=\"text-align: right;\">                9</td><td style=\"text-align: right;\">     96.99</td><td style=\"text-align: right;\">       0.05</td><td style=\"text-align: right;\">    0.91</td><td style=\"text-align: right;\">      0.09</td><td style=\"text-align: right;\"> 0.21</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_res_1 = predict_and_evaluate(xgb_clf_1, x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a5339-0f64-4b6d-87e8-40330544d140",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce401d34-9fe8-4bc3-a6c5-24f71efff0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf_1 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2712d4dc-f425-4611-9108-6277a4f8b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(x_train, y_train)\n",
    "svm_clf_1.fit(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a746ae93-d42b-45cb-a204-229d842977ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm  </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SVC        </td><td style=\"text-align: right;\">              898</td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">      98.4</td><td style=\"text-align: right;\">       0.09</td><td style=\"text-align: right;\">    0.89</td><td style=\"text-align: right;\">      0.16</td><td style=\"text-align: right;\"> 0.28</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_res = predict_and_evaluate(svm_clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f94d23d-f446-4b5f-a583-e95a6bf3cd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm  </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SVC        </td><td style=\"text-align: right;\">              719</td><td style=\"text-align: right;\">               13</td><td style=\"text-align: right;\">     98.71</td><td style=\"text-align: right;\">       0.11</td><td style=\"text-align: right;\">    0.87</td><td style=\"text-align: right;\">      0.19</td><td style=\"text-align: right;\">  0.3</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_res_1 = predict_and_evaluate(svm_clf_1, x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee77e5-f890-4203-9e74-35828c3711b8",
   "metadata": {},
   "source": [
    "## Comparing the metrics for all the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ce80aad-d0b5-4bed-b496-c6ef5e8ae7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=[rf_res, gb_res, xgb_res, svm_res], \n",
    "             columns=('Algorithm','False Positives', \n",
    "                      'False Negatives','Accuracy', 'Precision', \n",
    "                      'Recall', 'F1 Score', 'MCC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ac3dce1-2b88-44e4-8607-55fdc7567891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1123</td>\n",
       "      <td>9</td>\n",
       "      <td>98.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>2048</td>\n",
       "      <td>7</td>\n",
       "      <td>96.39</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>2600</td>\n",
       "      <td>9</td>\n",
       "      <td>95.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>898</td>\n",
       "      <td>11</td>\n",
       "      <td>98.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Algorithm  False Positives  False Negatives  Accuracy  \\\n",
       "0      RandomForestClassifier             1123                9     98.01   \n",
       "1  GradientBoostingClassifier             2048                7     96.39   \n",
       "2               XGBClassifier             2600                9     95.42   \n",
       "3                         SVC              898               11     98.40   \n",
       "\n",
       "   Precision  Recall  F1 Score   MCC  \n",
       "0       0.07    0.91      0.14  0.26  \n",
       "1       0.04    0.93      0.08  0.19  \n",
       "2       0.03    0.91      0.06  0.17  \n",
       "3       0.09    0.89      0.16  0.28  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0dd78d-834f-4905-a0be-5065aeb6de50",
   "metadata": {},
   "source": [
    "### For the Top 10 specific features of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "31603296-5a3c-4ac3-935e-de784d701726",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = pd.DataFrame(data=[rf_res1, gb_res_1, xgb_res_1, svm_res_1], \n",
    "             columns=('Algorithm','False Positives', \n",
    "                      'False Negatives', 'Accuracy', 'Precision', \n",
    "                      'Recall', 'F1 Score', 'MCC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e421f1e-ddac-4854-8bff-703fa05f3f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1058</td>\n",
       "      <td>10</td>\n",
       "      <td>98.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>1714</td>\n",
       "      <td>8</td>\n",
       "      <td>96.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>1705</td>\n",
       "      <td>9</td>\n",
       "      <td>96.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>719</td>\n",
       "      <td>13</td>\n",
       "      <td>98.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Algorithm  False Positives  False Negatives  Accuracy  \\\n",
       "0      RandomForestClassifier             1058               10     98.13   \n",
       "1  GradientBoostingClassifier             1714                8     96.98   \n",
       "2               XGBClassifier             1705                9     96.99   \n",
       "3                         SVC              719               13     98.71   \n",
       "\n",
       "   Precision  Recall  F1 Score   MCC  \n",
       "0       0.08    0.90      0.14  0.26  \n",
       "1       0.05    0.92      0.09  0.21  \n",
       "2       0.05    0.91      0.09  0.21  \n",
       "3       0.11    0.87      0.19  0.30  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978b8f1-6fdc-45fe-a33a-23c871157826",
   "metadata": {},
   "source": [
    "# Hypter-Parameter Tuning\n",
    "<b><l>Hyperparameter tuning refers to the shaping of the model architecture from the available space. This, in simple words, is nothing but searching for the night hyperparameter to find high precision and accuracy. Two of the most widely-used parameter optimiser techniques,</b></l> are : \n",
    " - <i>Grid Search</i>\n",
    " - <i>Random Search</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7be7508c-204e-4bbd-b108-c99635d02d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv(\"train_data.csv\")\n",
    "#test_df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22da3b58-584b-447d-bc54-7172f246fa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 31)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0ad0b2b8-abc5-466f-bb3a-a28deaf24eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop('Class', axis=1)\n",
    "y_train = train_df['Class']\n",
    "\n",
    "x_test = test_df.drop('Class', axis=1)\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fa0fc-2576-4fbf-a101-47ddb01bfcdc",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    " - In this tuning technique, we simply build a model for every combination of various hyperparameters and evaluate each model. The model which gives the highest accuracy wins.\n",
    " - The pattern followed here is similar to the grid, where all the values are placed in the form of a matrix.\n",
    " - Each set of parameters is taken into consideration and the accuracy is noted.\n",
    " - Once all the combinations are evaluated, the model with the set of parameters which give the top accuracy is considered to be the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8263ad8-9bae-4057-b87d-fda6758499bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " rf_params = {'n_estimators': [100, 200, 300, 500, 800],\n",
    "                'max_features': ['sqrt', 'log2'],\n",
    "                'max_depth': [8, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "               }\n",
    "\n",
    "# # Parameter matrix for Gradient Boost\n",
    " gbm_params = {  \"n_estimators\":[100,150,200],\n",
    "                 \"learning_rate\": [0.01, 0.025, 0.05],\n",
    "                 \"max_depth\":[3,5],\n",
    "                 \"subsample\":[ 0.8, 0.9,1.0], \n",
    "             }\n",
    "\n",
    " # Parameter matrix for XGBoost\n",
    "xgb_params = {'n_estimators' : [100,400,800],\n",
    "          \"learning_rate\"    : [0.01,0.05, 0.10] ,\n",
    "          \"max_depth\"        : [ 3, 5, 7, 13],\n",
    "          \"min_child_weight\" : [ 3, 5],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6f98394-cf92-40e4-84d2-09b96b885680",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'n_estimators' : [100, 200],\n",
    "              \"max_depth\"    : [ 3, 4],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97c1af0c-8a4b-4eb8-9e6b-9149825a1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_xgb = GridSearchCV(estimator= XGBClassifier(), \n",
    "                        param_grid= xgb_params, \n",
    "                        n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0c8610f7-11da-472a-93e7-3e77a5e029da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:25:15] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=4,\n",
       "             param_grid={'max_depth': [3, 4], 'n_estimators': [100, 200]})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b799eaa3-d603-4490-a738-f2d9ba614af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'n_estimators': 200}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b7d8428-3514-410e-b836-98d0a0985fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fa0d600-db0b-41b8-8c0d-a6d190d07769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm    </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>XGBClassifier</td><td style=\"text-align: right;\">                7</td><td style=\"text-align: right;\">               15</td><td style=\"text-align: right;\">     99.96</td><td style=\"text-align: right;\">       0.92</td><td style=\"text-align: right;\">    0.85</td><td style=\"text-align: right;\">      0.88</td><td style=\"text-align: right;\"> 0.88</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictons\n",
    "xgb_gs_hpo = predict_and_evaluate(gs_xgb.best_estimator_, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7816a-f7a7-4760-b0b9-2356ca6116c8",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "Random search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1e88045e-672b-462d-903c-fbfc09dfa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_xgb = RandomizedSearchCV(estimator= XGBClassifier(), \n",
    "                        param_distributions = xgb_params,\n",
    "                        n_iter= 2,\n",
    "                        n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c2976daa-3f34-4509-a566-ecaf0c06690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:31:01] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 5min 1s, sys: 1.83 s, total: 5min 3s\n",
      "Wall time: 5min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           random_state=None, reg_alpha=None,\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=2, n_jobs=2,\n",
       "                   param_distributions={'max_depth': [3, 4],\n",
       "                                        'n_estimators': [100, 200]})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rs_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3e2b1822-796b-4030-b007-f3c99e212dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200, 'max_depth': 4}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da2cc6be-eb35-4732-9341-2b5d82cc100b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1d82e759-f217-4f90-bfe5-9341a52b6352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Algorithm    </th><th style=\"text-align: right;\">  False Positives</th><th style=\"text-align: right;\">  False Negatives</th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  MCC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>XGBClassifier</td><td style=\"text-align: right;\">                7</td><td style=\"text-align: right;\">               15</td><td style=\"text-align: right;\">     99.96</td><td style=\"text-align: right;\">       0.92</td><td style=\"text-align: right;\">    0.85</td><td style=\"text-align: right;\">      0.88</td><td style=\"text-align: right;\"> 0.88</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictons\n",
    "xgb_rs_hpo = predict_and_evaluate(gs_xgb.best_estimator_, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dad9db-c304-4101-826d-450801c9860d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc644bdd-7bbe-4577-9a7f-6224952454cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
