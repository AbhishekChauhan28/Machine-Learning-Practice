{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSPCom-KmApV",
    "tags": []
   },
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLGkt5qiyz4E",
    "tags": []
   },
   "source": [
    "- This Example demonstrates training a simple **Convolutional Neural Network(CNN)** to classify [CIFAR images](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "- The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n",
    "\n",
    "\n",
    "<img src=\"https://paperswithcode.com/media/datasets/CIFAR-10-0000000431-b71f61c0_U5n3Glr.jpg\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRFxccghyMVo",
    "tags": []
   },
   "source": [
    "## Download and prepare the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JWoEqyMuXFF4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[34, 36, 43],\n",
       "        [37, 37, 51],\n",
       "        [44, 44, 64],\n",
       "        ...,\n",
       "        [62, 73, 93],\n",
       "        [52, 63, 83],\n",
       "        [45, 57, 77]],\n",
       "\n",
       "       [[35, 35, 37],\n",
       "        [34, 34, 40],\n",
       "        [34, 33, 43],\n",
       "        ...,\n",
       "        [58, 69, 89],\n",
       "        [50, 60, 80],\n",
       "        [47, 58, 78]],\n",
       "\n",
       "       [[34, 33, 35],\n",
       "        [33, 32, 36],\n",
       "        [33, 32, 38],\n",
       "        ...,\n",
       "        [66, 77, 97],\n",
       "        [51, 62, 82],\n",
       "        [53, 64, 84]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[32, 32, 38],\n",
       "        [32, 31, 37],\n",
       "        [33, 33, 38],\n",
       "        ...,\n",
       "        [41, 38, 45],\n",
       "        [34, 31, 38],\n",
       "        [36, 32, 40]],\n",
       "\n",
       "       [[33, 31, 36],\n",
       "        [32, 30, 36],\n",
       "        [34, 32, 37],\n",
       "        ...,\n",
       "        [37, 35, 40],\n",
       "        [32, 29, 35],\n",
       "        [30, 28, 33]],\n",
       "\n",
       "       [[37, 32, 35],\n",
       "        [37, 32, 36],\n",
       "        [37, 33, 36],\n",
       "        ...,\n",
       "        [49, 47, 49],\n",
       "        [47, 45, 48],\n",
       "        [58, 56, 58]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JWoEqyMuXFF4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [2],\n",
       "       [5],\n",
       "       ...,\n",
       "       [8],\n",
       "       [8],\n",
       "       [8]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,)\n",
    "y_val = y_val.reshape(-1,)\n",
    "y_test = y_test.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 5, ..., 8, 8, 8], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wArwCTJJlUa",
    "tags": []
   },
   "source": [
    "## Verify the data\n",
    "\n",
    "To verify that the dataset looks correct, let's plot some images from the training set and display the class name below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "K3PAELE2eSU9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def plot_sample(X, y, index):\n",
    "    plt.figure(figsize = (15,2))\n",
    "    plt.imshow(X[index])\n",
    "    plt.xlabel(class_names[y[index]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(X_train, y_train, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convolutional Neural Network\n",
    "<img src=\"https://miro.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oewp-wYg31t9",
    "tags": []
   },
   "source": [
    "### Create the convolutional base (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hQvqXpNyN3x",
    "tags": []
   },
   "source": [
    "The 6 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers.\n",
    "\n",
    "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure your CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument `input_shape` to your first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "L9YmGQBQPrdn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "L9YmGQBQPrdn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "L9YmGQBQPrdn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvDVFkg-2DPm",
    "tags": []
   },
   "source": [
    "Let's display the architecture of your model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8-C4XBg4UTJy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "=================================================================\n",
      "Total params: 19,392\n",
      "Trainable params: 19,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j-AXYeZ2GO5",
    "tags": []
   },
   "source": [
    "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically,  as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v8sVOtG37bT",
    "tags": []
   },
   "source": [
    "### Add Dense layers on top (Classification)\n",
    "\n",
    "To complete the model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mRs95d6LUVEi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipGiQMcR4Gtq",
    "tags": []
   },
   "source": [
    "Here's the complete architecture of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8Yu_m-TZUWGX",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 167,562\n",
      "Trainable params: 167,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNKXi-Gy3RO-",
    "tags": []
   },
   "source": [
    "The network summary shows that (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3odqfHP4M67",
    "tags": []
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MdDzI75PUXrG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MdDzI75PUXrG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4929: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.5305 - accuracy: 0.4478 - val_loss: 1.2504 - val_accuracy: 0.5592\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1754 - accuracy: 0.5883 - val_loss: 1.1910 - val_accuracy: 0.5843\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.0416 - accuracy: 0.6388 - val_loss: 1.0534 - val_accuracy: 0.6318\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.9510 - accuracy: 0.6676 - val_loss: 0.9815 - val_accuracy: 0.6579\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8835 - accuracy: 0.6946 - val_loss: 0.9495 - val_accuracy: 0.6696\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.8291 - accuracy: 0.7116 - val_loss: 0.9594 - val_accuracy: 0.6718\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7801 - accuracy: 0.7286 - val_loss: 0.9422 - val_accuracy: 0.6755\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.7355 - accuracy: 0.7456 - val_loss: 0.9231 - val_accuracy: 0.6855\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6961 - accuracy: 0.7570 - val_loss: 0.9523 - val_accuracy: 0.6776\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6591 - accuracy: 0.7699 - val_loss: 0.9722 - val_accuracy: 0.6731\n",
      "CPU times: user 17min 25s, sys: 25.6 s, total: 17min 50s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%time history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKgyC5K_4O0d",
    "tags": []
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtyDF0MKUcM7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtyDF0MKUcM7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LvwaKhtUdOo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
